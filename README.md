# 21-brain: Learning Blackjack with Double Q-Learning and Card Counting

## ğŸ§  Overview

This project explores reinforcement learning (RL) applied to the game of Blackjack. We develop an OpenAI Gym-compatible Blackjack environment and train intelligent agents using **Double Q-Learning**, incorporating complex decision mechanics like **doubling**, **splitting**, and **card counting** (Hi-Lo system).

Our goal is to answer:  
**Can Double Q-Learning effectively learn Blackjack strategies, including betting and count-based adaptations?**

## ğŸ¯ Key Features

- ğŸƒ Custom Gym environment implementing full Blackjack rules (hit, stand, double, split, betting)
- ğŸ”¢ Card counting support using **true count** (Hi-Lo)
- ğŸ¤– Double Q-Learning agents trained over four progressive phases
- ğŸ“ˆ Performance evaluated against the Basic Strategy with visual heatmaps and metrics

## ğŸ› ï¸ Environment

The Blackjack simulation supports:

- Variable number of decks and reshuffling threshold
- Full action space: `hit`, `stand`, `double`, `split`, and bet sizing
- Card counting using true count as part of the observation space

### Observation Space

- Player score (1â€“31)
- Dealer upcard (1â€“10)
- Soft hand indicator (1 if soft hand, 0 otherwise)
- Indicator for ability to split
- Pair value (if applicable)
- True count (normalized by decks remaining)

### Action Space

1. **Betting Phase**: Select bet size based on true count  
2. **Gameplay Phase**: Choose among `hit`, `stand`, `double`, `split`

### Rewards

- Win: +1  
- Loss: -1  
- Natural Blackjack: +1.5  
- Rewards from `double` or `split` scale according to outcome

## ğŸ“š Training Pipeline

Training is divided into 4 stages, each implemented in its own Jupyter notebook:

1. `dq_hit_stand.ipynb`: Learn `hit` and `stand` actions
2. `dq_hit_stand_dd.ipynb`: Add support for `double down`
3. `betting.ipynb`: Train dynamic betting policy using card count
4. `split.ipynb`: Add support for `split` and multi-hand logic

Each stage builds upon the previous Q-table and training knowledge to incrementally improve the policy.

## ğŸ“Š Results

| Strategy             | Win Rate | Draw Rate | Loss Rate | Avg. Reward |
|----------------------|----------|-----------|-----------|-------------|
| Stage 1: Hit/Stand   | 43.2%    | 8.7%      | 48.1%     | -0.0258     |
| Stage 2: +Double     | 43.2%    | 8.6%      | 48.2%     | -0.0092     |
| Stage 3: +Betting    | 43.3%    | 8.6%      | 48.1%     | -0.0044     |
| Stage 4: +Split      | 43.4%    | 8.9%      | 47.7%     | **+0.0148** |

Agents learn to closely follow the Basic Strategy, adjust bets according to favorable deck conditions, and achieve positive returns in later stages.

## ğŸ“ Repository Structure

```
21-brain/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ env.py                 # Custom Gym environment
â”‚   â”œâ”€â”€ utils.py               # Helper functions for training 
â”‚   â””â”€â”€ blackjack.py           # Setup classes for blackjack game
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ dq_hit_stand.ipynb     # Stage 1: Hit/Stand training
â”‚   â”œâ”€â”€ dq_hit_stand_dd.ipynb  # Stage 2: Add Double Down
â”‚   â”œâ”€â”€ betting.ipynb          # Stage 3: Betting strategy
â”‚   â””â”€â”€ split.ipynb            # Stage 4: Add Split logic
â”œâ”€â”€ strategies/                # Learnt strategies in CSV files 
â”œâ”€â”€ README.md
â”œâ”€â”€ 21_brain.pdf               # Report of the project                   
â”œâ”€â”€ requirements.txt           
â””â”€â”€ project_rules.pdf          
```


## ğŸ§ª Future Work

- Multi-player Blackjack environment
- Finer-grained or continuous bet sizing
- Curriculum-based training (simple to complex hands)
- Deep Q-Networks (DQN) for large state generalization
