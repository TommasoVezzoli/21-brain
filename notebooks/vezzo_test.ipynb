{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "#from file env.py in directory src (you need to change directory)\n",
    "os.chdir('..')\n",
    "os.chdir('src')\n",
    "from env import BlackjackEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_decks\": 6,\n",
    "    \"red_card_position\": 0.2,\n",
    "    \"bet_size\": [1],\n",
    "    \"actions\": [\"stand\", \"hit\"],\n",
    "    \"num_players\": 1\n",
    "}\n",
    "# Create environment with 6 decks (standard casino configuration)\n",
    "env = BlackjackEnv(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relevant states - focusing on decision points that matter\n",
    "states = []\n",
    "# For hard totals (no usable ace), only track 12-21\n",
    "# Below 12, the optimal play is always hit\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(2, 12):\n",
    "        states.append((player_sum, dealer_card, 0))  # Hard total\n",
    "\n",
    "# For soft totals (with usable ace), track 12-21\n",
    "# Soft totals below 12 are impossible (A+1 = 12)\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(2, 12):\n",
    "        states.append((player_sum, dealer_card, 1))  # Soft total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Q-table with strategic initial values\n",
    "q_data = {\n",
    "    'State': states,\n",
    "    'Action 0 (Stand)': np.zeros(len(states)),\n",
    "    'Action 1 (Hit)': np.zeros(len(states))\n",
    "}\n",
    "\n",
    "# Strategic initialization: Set high values for \"stand\" in 20-21, high values for \"hit\" in 4-11\n",
    "for i, state in enumerate(states):\n",
    "    player_sum, _, _ = state\n",
    "    if player_sum >= 20:\n",
    "        # For high player sums, initialize stand value higher\n",
    "        q_data['Action 0 (Stand)'][i] = 0.5\n",
    "        q_data['Action 1 (Hit)'][i] = -0.1\n",
    "    elif player_sum < 12:\n",
    "        # For low player sums, initialize hit value higher\n",
    "        q_data['Action 0 (Stand)'][i] = -0.1\n",
    "        q_data['Action 1 (Hit)'][i] = 0.5\n",
    "\n",
    "Q = pl.DataFrame(q_data)\n",
    "\n",
    "# Double Q-learning: Second Q-table for reducing bias\n",
    "Q2 = Q.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Count-Bet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ranges = [[0], [1,2,3],[4]]\n",
    "\n",
    "count_data = {\n",
    "    'True Count' : count_ranges,\n",
    "    'Low Bet (1)' : np.zeros(len(count_ranges)),\n",
    "    'Mid Bet (2)' : np.zeros(len(count_ranges)),\n",
    "    'High Bet (5)' : np.zeros(len(count_ranges))\n",
    "}\n",
    "# Initialize betting strategy for each count range, which are fixed in all matches\n",
    "for i, count_range in enumerate(count_ranges):\n",
    "    if count_range == [0]:\n",
    "        count_data['Low Bet (1)'][i] = 0.5\n",
    "        count_data['Mid Bet (2)'][i] = 0.1\n",
    "        count_data['High Bet (5)'][i] = 0.1\n",
    "    elif count_range == [1,2,3]:\n",
    "        count_data['Low Bet (1)'][i] = 0.2\n",
    "        count_data['Mid Bet (2)'][i] = 0.5\n",
    "        count_data['High Bet (5)'][i] = 0.2\n",
    "    else:\n",
    "        count_data['Low Bet (1)'][i] = 0.2\n",
    "        count_data['Mid Bet (2)'][i] = 0.3\n",
    "        count_data['High Bet (5)'][i] = 0.5\n",
    "\n",
    "\n",
    "count_df = pl.DataFrame(count_data)\n",
    "\n",
    "# use a dictionary which associates each column name of the type of bet to the actual bet value\n",
    "betting_strategy = {\n",
    "    'Low Bet (1)': 1,\n",
    "    'Mid Bet (2)': 2,\n",
    "    'High Bet (5)': 5\n",
    "}\n",
    "\n",
    "# Track state-action visit counts for adaptive learning rates\n",
    "visit_counts = {}\n",
    "for state in states:\n",
    "    visit_counts[(state, 0)] = 0  # Stand\n",
    "    visit_counts[(state, 1)] = 0  # Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Improved hyperparameters\n",
    "initial_lr = 0.1             # Learning rate\n",
    "lr_decay_rate = 0.00005      # Gentler decay rate\n",
    "gamma = 0.95                 # Higher discount factor - long-term rewards matter more\n",
    "n_episodes = 200000         # More training episodes\n",
    "initial_epsilon = 1.0        # Start with 100% exploration\n",
    "epsilon_min = 0.01           # Minimum exploration rate\n",
    "epsilon_decay = 0.99995      # Much slower decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified state representation - focusing on meaningful game states\n",
    "def get_state_features(full_state):\n",
    "    # Extract just player sum, dealer card, and usable ace\n",
    "    player_sum = full_state[0]\n",
    "    dealer_card = full_state[1]\n",
    "    usable_ace = full_state[2]\n",
    "    return (player_sum, dealer_card, usable_ace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_lr(state, action, base_lr):\n",
    "    \"\"\"Get state-action specific learning rate based on visit count\"\"\"\n",
    "    key = (state, action)\n",
    "    count = visit_counts.get(key, 0) + 1\n",
    "    # Decay learning rate based on visit count, but maintain a minimum rate\n",
    "    return max(base_lr / (1 + 0.005 * count), base_lr * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(state_features, q_table=Q):\n",
    "    \"\"\"Get Q-values for a given state\"\"\"\n",
    "    # Filter the DataFrame for the specific state\n",
    "    state_row = q_table.filter(pl.col('State') == state_features)\n",
    "    \n",
    "    if len(state_row) == 0:\n",
    "        # Return default values based on player sum\n",
    "        player_sum = state_features[0]\n",
    "        if player_sum < 12:\n",
    "            return np.array([-0.1, 0.5])  # Default to hit for low sums\n",
    "        elif player_sum >= 20:\n",
    "            return np.array([0.5, -0.1])  # Default to stand for high sums\n",
    "        else:\n",
    "            return np.array([0.0, 0.0])  # Neutral for middle sums\n",
    "            \n",
    "    # Extract Q-values from the DataFrame\n",
    "    stand_val = state_row.select('Action 0 (Stand)').item()\n",
    "    hit_val = state_row.select('Action 1 (Hit)').item()\n",
    "    return np.array([stand_val, hit_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(state_features, action, reward, next_state_features, lr, q_table=Q, q_table_target=Q2):\n",
    "    \"\"\"Update Q-value for state-action pair using Double Q-learning\"\"\"\n",
    "    # Check if state exists in our table\n",
    "    state_row = q_table.filter(pl.col('State') == state_features)\n",
    "    if len(state_row) == 0:\n",
    "        return # State not in our table\n",
    "    \n",
    "    # Determine which action column to update\n",
    "    action_col = 'Action 1 (Hit)' if action == 1 else 'Action 0 (Stand)'\n",
    "    \n",
    "    # Current Q-value in the DataFrame\n",
    "    current_q = state_row.select(action_col).item()\n",
    "    \n",
    "    # If next_state_features is None, this is a terminal state\n",
    "    if next_state_features is None:\n",
    "        # Terminal state - no future rewards\n",
    "        new_q = current_q + lr * (reward - current_q)\n",
    "    else:\n",
    "        # Get the next state's best action from current Q-table\n",
    "        next_q_values = get_q_values(next_state_features, q_table)\n",
    "        best_next_action = np.argmax(next_q_values)\n",
    "        \n",
    "        # Get Q-value for best action from target Q-table\n",
    "        next_q_values_target = get_q_values(next_state_features, q_table_target)\n",
    "        max_next_q = next_q_values_target[best_next_action]\n",
    "        \n",
    "        # Q-learning update formula with future rewards\n",
    "        new_q = current_q + lr * (reward + gamma * max_next_q - current_q)\n",
    "    \n",
    "    # Update the Q-table entry in the DataFrame\n",
    "    # Create a temporary mask for the state we want to update\n",
    "    mask = pl.col('State') == state_features\n",
    "    \n",
    "    # Use the when/then/otherwise pattern to update values\n",
    "    q_table = q_table.with_columns(\n",
    "        pl.when(mask)\n",
    "        .then(pl.lit(new_q))\n",
    "        .otherwise(pl.col(action_col))\n",
    "        .alias(action_col)\n",
    "    )\n",
    "    \n",
    "    # Track visit counts\n",
    "    visit_counts[(state_features, action)] = visit_counts.get((state_features, action), 0) + 1\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_count_table(current_count, reward, count_df=count_df):\n",
    "    \"\"\"Update the value associated with the move of betting a certain amount based on the reward received\"\"\"\n",
    "    print(count_df)\n",
    "    # Check if reward is positive or negative\n",
    "    if reward > 0:\n",
    "        # Positive reward - increase the value of betting a higher amount and decrease the value of betting a lower amount for the row corresponding to the current count\n",
    "        # Find the index of the current count range\n",
    "        index = None\n",
    "        if current_count <= 0:\n",
    "            index = 0\n",
    "        elif current_count <= 3:\n",
    "            index = 1\n",
    "        else:\n",
    "            index = 2\n",
    "        # update the row corresponding to the current count\n",
    "        if index is None:\n",
    "            raise ValueError(\"Current count not found in count ranges\")\n",
    "        # Update the values in the count_df DataFrame\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Low Bet (1)'][index] * 0.9))\n",
    "            .otherwise(pl.col('Low Bet (1)'))\n",
    "            .alias('Low Bet (1)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Mid Bet (2)'][index] * 0.95))\n",
    "            .otherwise(pl.col('Mid Bet (2)'))\n",
    "            .alias('Mid Bet (2)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['High Bet (5)'][index] * 1.05))\n",
    "            .otherwise(pl.col('High Bet (5)'))\n",
    "            .alias('High Bet (5)')\n",
    "        )\n",
    "    elif reward < 0:\n",
    "        # Negative reward - decrease the value of betting a higher amount and increase the value of betting a lower amount for the row corresponding to the current count\n",
    "        # Find the index of the current count range\n",
    "        index = None\n",
    "        if current_count <= 0:\n",
    "            index = 0\n",
    "        elif current_count <= 3:\n",
    "            index = 1\n",
    "        else:\n",
    "            index = 2\n",
    "        # update the row corresponding to the current count\n",
    "        if index is None:\n",
    "            raise ValueError(\"Current count not found in count ranges\")\n",
    "        # Update the values in the count_df DataFrame\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Low Bet (1)'][index] * 1.05))\n",
    "            .otherwise(pl.col('Low Bet (1)'))\n",
    "            .alias('Low Bet (1)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Mid Bet (2)'][index] * 0.95))\n",
    "            .otherwise(pl.col('Mid Bet (2)'))\n",
    "            .alias('Mid Bet (2)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['High Bet (5)'][index] * 0.9))\n",
    "            .otherwise(pl.col('High Bet (5)'))\n",
    "            .alias('High Bet (5)')\n",
    "        )\n",
    "    else:\n",
    "        #do nothing\n",
    "        #ALTERNATIVE: update something\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved training...\n",
      "Episode 10000, max Q-value change: 0.790221\n",
      "Episode 20000, max Q-value change: 0.464655\n",
      "Episode 30000, max Q-value change: 0.285216\n",
      "Episode 40000, max Q-value change: 0.196771\n",
      "Episode 50000, max Q-value change: 0.200968\n",
      "Episode 60000, max Q-value change: 0.123454\n",
      "Episode 70000, max Q-value change: 0.108336\n",
      "Episode 80000, max Q-value change: 0.093450\n",
      "Episode 90000, max Q-value change: 0.109359\n",
      "Episode 100000, max Q-value change: 0.084005\n",
      "Episode 110000, max Q-value change: 0.073266\n",
      "Episode 120000, max Q-value change: 0.046212\n",
      "Episode 130000, max Q-value change: 0.049739\n",
      "Episode 140000, max Q-value change: 0.042723\n",
      "Episode 150000, max Q-value change: 0.033059\n",
      "Episode 160000, max Q-value change: 0.034446\n",
      "Episode 170000, max Q-value change: 0.040687\n",
      "Episode 180000, max Q-value change: 0.030728\n",
      "Episode 190000, max Q-value change: 0.030142\n",
      "Training complete after 200000 episodes.\n",
      "Win rate: 0.4072\n",
      "Draw rate: 0.0625\n",
      "Loss rate: 0.5303\n"
     ]
    }
   ],
   "source": [
    "# Training loop with convergence check\n",
    "print(\"Starting improved training...\")\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "epsilon = initial_epsilon\n",
    "lr = initial_lr\n",
    "money_won = 0\n",
    "money_lost = 0\n",
    "\n",
    "# Parameters for convergence\n",
    "n_episodes = 200000  # Number of episodes for training\n",
    "convergence_threshold = 0.001  # Lower threshold for better stability\n",
    "convergence_check_interval = 10000  # Check for convergence every N episodes\n",
    "convergence_required_count = 3  # Number of consecutive checks below threshold to confirm convergence\n",
    "max_episodes = n_episodes  # Maximum episodes as a fallback\n",
    "\n",
    "# Keep a copy of the previous Q-table for comparison\n",
    "previous_q = Q.clone()\n",
    "convergence_count = 0\n",
    "converged = False\n",
    "episode = 0\n",
    "#first training phase only for the Q-table with fixed betting strategy\n",
    "while episode < max_episodes and not converged:\n",
    "\n",
    "    env.reset()\n",
    "    bet_index = env.bet_space.sample()  # Sample bet index from the environment\n",
    "    bet_amount = env.bets[bet_index]  # Sample bet amount from the environment\n",
    "    # print(env.step(bet_index, action_type=\"bet\"))\n",
    "    state, reward, done = env.step(bet_index, action_type=\"bet\")  # Place bet\n",
    "    if done:\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "            money_won += reward * bet_amount\n",
    "        elif reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "            money_lost += abs(reward) * bet_amount\n",
    "    # print(bet_amount)\n",
    "    state_features = get_state_features(state)\n",
    "\n",
    "    # Training episode\n",
    "    while not done:\n",
    "        \n",
    "        if state_features[0] < 12:\n",
    "        # Always hit this state as it's not relevant for our training\n",
    "            next_state, _, _ = env.step(1, action_type=\"move\")\n",
    "            next_state_features = get_state_features(next_state) if not done else None\n",
    "            state = next_state\n",
    "            state_features = next_state_features if next_state is not None else None\n",
    "            continue\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        elif np.random.rand() < epsilon:\n",
    "            action = env.move_space.sample()  # Random action\n",
    "        else:\n",
    "            q_values = get_q_values(state_features)\n",
    "            action = np.argmax(q_values)  # Greedy action\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action, action_type=\"move\")\n",
    "        next_state_features = get_state_features(next_state) if not done else None\n",
    "\n",
    "        # Get adaptive learning rate for this state-action pair\n",
    "        adaptive_lr = get_adaptive_lr(state_features, action, lr)\n",
    "\n",
    "        # Randomly decide which Q-table to update (Double Q-learning)\n",
    "        # print(f\"State: {state_features}, Action: {action}, Done: {done}, Reward: {reward}, Next State: {next_state_features}\")\n",
    "        if np.random.rand() < 0.5:\n",
    "            # print(\"Updating Q-table 1\")\n",
    "            Q = update_q_value(state_features, action, reward*bet_amount, next_state_features, adaptive_lr, Q, Q2)\n",
    "        else:\n",
    "            # print(\"Updating Q-table 2\")\n",
    "            Q2 = update_q_value(state_features, action, reward*bet_amount, next_state_features, adaptive_lr, Q2, Q)\n",
    "            \n",
    "        # Track outcomes\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                wins += 1\n",
    "                money_won += reward * bet_amount\n",
    "            elif reward == 0:\n",
    "                draws += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "                money_lost += abs(reward) * bet_amount\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = next_state_features if next_state is not None else None\n",
    "        \n",
    "        if state_features is None:\n",
    "            # print(f\"Entered break condition with done being {done}\")\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon and learning rate\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    lr = initial_lr / (1 + lr_decay_rate * episode)\n",
    "    \n",
    "    # Check for convergence periodically\n",
    "    if episode % convergence_check_interval == 0 and episode > 0:\n",
    "        # Calculate the maximum absolute difference between current and previous Q-values\n",
    "        diff_stand = (Q.select('Action 0 (Stand)').to_numpy() - \n",
    "                     previous_q.select('Action 0 (Stand)').to_numpy())\n",
    "        diff_hit = (Q.select('Action 1 (Hit)').to_numpy() - \n",
    "                   previous_q.select('Action 1 (Hit)').to_numpy())\n",
    "        \n",
    "        max_diff_stand = np.max(np.abs(diff_stand))\n",
    "        max_diff_hit = np.max(np.abs(diff_hit))\n",
    "        max_diff = max(max_diff_stand, max_diff_hit)\n",
    "        \n",
    "        if max_diff < convergence_threshold:\n",
    "            convergence_count += 1\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f} (convergence count: {convergence_count}/{convergence_required_count})\")\n",
    "            if convergence_count >= convergence_required_count:\n",
    "                print(f\"Converged after {episode} episodes (max Q-value change: {max_diff:.6f})\")\n",
    "                converged = True\n",
    "        else:\n",
    "            convergence_count = 0\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f}\")\n",
    "        \n",
    "        # Store current Q-values for next comparison\n",
    "        previous_q = Q.clone()\n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# Final statistics\n",
    "total_episodes = episode\n",
    "print(f\"Training complete after {total_episodes} episodes.\")\n",
    "print(f\"Win rate: {wins/total_episodes:.4f}\")\n",
    "print(f\"Draw rate: {draws/total_episodes:.4f}\")\n",
    "print(f\"Loss rate: {losses/total_episodes:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Q-table win rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final policy evaluation...\n",
      "Final evaluation complete.\n",
      "Win rate: 0.4336\n",
      "Draw rate: 0.0871\n",
      "Loss rate: 0.4793\n",
      "Money won: 4550.0\n",
      "Money lost: 4793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the final policy with more episodes\n",
    "print(\"\\nFinal policy evaluation...\")\n",
    "eval_wins = 0\n",
    "eval_draws = 0\n",
    "eval_loss = 0\n",
    "money_won = 0\n",
    "money_lost = 0\n",
    "eval_episodes = 10000\n",
    "\n",
    "for _ in range(eval_episodes):\n",
    "    env.reset()\n",
    "    bet_index = env.bet_space.sample()  # Sample bet index from the environment\n",
    "    bet_amount = env.bets[bet_index]  # Sample bet amount from the environment\n",
    "    # print(env.step(bet_index, action_type=\"bet\"))\n",
    "    state, reward, done = env.step(bet_index, action_type=\"bet\")  # Place bet\n",
    "    if done:\n",
    "        if reward > 0:\n",
    "            eval_wins += 1\n",
    "            money_won += reward * bet_amount\n",
    "        elif reward == 0:\n",
    "            eval_draws += 1\n",
    "        else:\n",
    "            eval_loss += 1\n",
    "            money_lost += abs(reward) * bet_amount\n",
    "    # print(bet_amount)\n",
    "    state_features = get_state_features(state)\n",
    "\n",
    "    # Training episode\n",
    "    while not done:\n",
    "        # Always choose the best action according to average of both Q-tables\n",
    "        q_values1 = get_q_values(state_features, Q)\n",
    "        q_values2 = get_q_values(state_features, Q2)\n",
    "        avg_q_values = (q_values1 + q_values2) / 2\n",
    "        action = np.argmax(avg_q_values)\n",
    "        \n",
    "        next_state, reward, done = env.step(action, action_type=\"move\")\n",
    "        \n",
    "        if done and reward > 0:\n",
    "            eval_wins += 1\n",
    "            money_won += reward * bet_amount\n",
    "        elif done and reward == 0:\n",
    "            eval_draws += 1\n",
    "        elif done and reward < 0:\n",
    "            eval_loss += 1\n",
    "            money_lost += abs(reward) * bet_amount\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = get_state_features(state)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(f\"Final evaluation complete.\")\n",
    "print(f\"Win rate: {eval_wins/eval_episodes:.4f}\")\n",
    "print(f\"Draw rate: {eval_draws/eval_episodes:.4f}\")\n",
    "print(f\"Loss rate: {eval_loss/eval_episodes:.4f}\")\n",
    "print(f\"Money won: {money_won}\")\n",
    "print(f\"Money lost: {money_lost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Count-Bet table (Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with convergence check\n",
    "print(\"Starting improved training...\")\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "money_won = 0\n",
    "money_lost = 0\n",
    "\n",
    "# Parameters for convergence\n",
    "convergence_threshold = 0.001  # Lower threshold for better stability\n",
    "convergence_check_interval = 10000  # Check for convergence every N episodes\n",
    "convergence_required_count = 3  # Number of consecutive checks below threshold to confirm convergence\n",
    "max_episodes = n_episodes  # Maximum episodes as a fallback\n",
    "\n",
    "# Keep a copy of the previous Q-table for comparison\n",
    "previous_count = count_df.clone()\n",
    "convergence_count = 0\n",
    "converged = False\n",
    "episode = 0\n",
    "\n",
    "#training phase for the betting strategy\n",
    "while episode < max_episodes and not converged:\n",
    "\n",
    "    obs = env.reset()\n",
    "    current_count = obs[3]\n",
    "    bet_index = env.bet_space.sample()\n",
    "    bet_amount = env.bets[bet_index]\n",
    "    state, reward, done = env.step(bet_index, action_type=\"bet\") \n",
    "\n",
    "    # Find the index of the current count range\n",
    "    if current_count <= 0:\n",
    "        index = 0\n",
    "    elif current_count <= 3:\n",
    "        index = 1\n",
    "    else:\n",
    "        index = 2\n",
    "    # Get the row for current count\n",
    "    count_row = count_df.filter(pl.col('True Count') == count_ranges[index])\n",
    "\n",
    "    # Get values for each bet type\n",
    "    low_bet_val = count_row.select('Low Bet (1)').item()\n",
    "    mid_bet_val = count_row.select('Mid Bet (2)').item()\n",
    "    high_bet_val = count_row.select('High Bet (5)').item()\n",
    "\n",
    "    # Find column with maximum value\n",
    "    if low_bet_val >= mid_bet_val and low_bet_val >= high_bet_val:\n",
    "        max_bet_col = 'Low Bet (1)'\n",
    "    elif mid_bet_val >= low_bet_val and mid_bet_val >= high_bet_val:\n",
    "        max_bet_col = 'Mid Bet (2)'\n",
    "    else:\n",
    "        max_bet_col = 'High Bet (5)'\n",
    "\n",
    "    # Get the bet amount\n",
    "    bet_amount = betting_strategy[max_bet_col]\n",
    "\n",
    "    if done:\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "            money_won += reward * bet_amount\n",
    "        elif reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "            money_lost += abs(reward) * bet_amount\n",
    "    \n",
    "    state_features = get_state_features(state)\n",
    "\n",
    "    # Training episode\n",
    "    while not done:\n",
    "\n",
    "        if state_features[0] < 12:\n",
    "        # Always hit this state as it's not relevant for our training\n",
    "            next_state, _, _ = env.step(1, action_type=\"move\")\n",
    "            next_state_features = get_state_features(next_state) if not done else None\n",
    "            state = next_state\n",
    "            state_features = next_state_features if next_state is not None else None\n",
    "            continue\n",
    "        \n",
    "        q_values1 = get_q_values(state_features, Q)\n",
    "        q_values2 = get_q_values(state_features, Q2)\n",
    "        avg_q_values = (q_values1 + q_values2) / 2\n",
    "        action = np.argmax(avg_q_values)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action, action_type=\"move\")\n",
    "        next_state_features = get_state_features(next_state) if not done else None\n",
    "        \n",
    "        #update the count_df DataFrame based on the reward received\n",
    "        print(current_count)\n",
    "        count_df = update_count_table(current_count, reward*bet_amount, count_df)\n",
    "            \n",
    "        # Track outcomes\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                wins += 1\n",
    "                money_won += reward * bet_amount\n",
    "            elif reward == 0:\n",
    "                draws += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "                money_lost += abs(reward) * bet_amount\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = next_state_features if next_state is not None else None\n",
    "        \n",
    "        if state_features is None:\n",
    "            break\n",
    "    #check for convergence of the count_df DataFrame\n",
    "    if episode % convergence_check_interval == 0 and episode > 0:\n",
    "        # Calculate the maximum absolute difference between current and previous Q-values\n",
    "        diff_stand = (count_df.select('Low Bet (1)').to_numpy() - \n",
    "                     previous_count.select('Low Bet (1)').to_numpy())\n",
    "        diff_hit = (count_df.select('Mid Bet (2)').to_numpy() - \n",
    "                   previous_count.select('Mid Bet (2)').to_numpy())\n",
    "        \n",
    "        max_diff_stand = np.max(np.abs(diff_stand))\n",
    "        max_diff_hit = np.max(np.abs(diff_hit))\n",
    "        max_diff = max(max_diff_stand, max_diff_hit)\n",
    "        \n",
    "        if max_diff < convergence_threshold:\n",
    "            convergence_count += 1\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f} (convergence count: {convergence_count}/{convergence_required_count})\")\n",
    "            if convergence_count >= convergence_required_count:\n",
    "                print(f\"Converged after {episode} episodes (max Q-value change: {max_diff:.6f})\")\n",
    "                converged = True\n",
    "        else:\n",
    "            convergence_count = 0\n",
    "            print(f\"Episode {episode}, max value change: {max_diff:.6f}\")\n",
    "        \n",
    "        # Store current Q-values for next comparison\n",
    "        previous_count = count_df.clone()\n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# Final statistics\n",
    "total_episodes = episode\n",
    "print(f\"Training complete after {total_episodes} episodes.\")\n",
    "print(f\"Win rate: {wins/total_episodes:.4f}\")\n",
    "print(f\"Draw rate: {draws/total_episodes:.4f}\")\n",
    "print(f\"Loss rate: {losses/total_episodes:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Q + Count-bet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average Q-values (ensemble approach)\n",
    "Qp = Q.to_pandas()\n",
    "Q2p = Q2.to_pandas()\n",
    "avg_Q = Qp.copy()\n",
    "avg_Q['Action 0 (Stand)'] = (Qp['Action 0 (Stand)'] + Q2p['Action 0 (Stand)']) / 2\n",
    "avg_Q['Action 1 (Hit)'] = (Qp['Action 1 (Hit)'] + Q2p['Action 1 (Hit)']) / 2\n",
    "avg_Q['Best Action'] = avg_Q.apply(\n",
    "    lambda row: \"Stand\" if row['Action 0 (Stand)'] > row['Action 1 (Hit)'] else \"Hit\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
