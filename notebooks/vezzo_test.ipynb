{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "#from file env.py in directory src (you need to change directory)\n",
    "os.chdir('..')\n",
    "os.chdir('src')\n",
    "from env import BlackjackEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_decks\": 6,\n",
    "    \"red_card_position\": 0.2,\n",
    "    \"bet_size\": [1],\n",
    "    \"actions\": [\"stand\", \"hit\"],\n",
    "    \"num_players\": 1\n",
    "}\n",
    "# Create environment with 6 decks (standard casino configuration)\n",
    "env = BlackjackEnv(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relevant states - focusing on decision points that matter\n",
    "states = []\n",
    "# For hard totals (no usable ace), only track 12-21\n",
    "# Below 12, the optimal play is always hit\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(1, 11):\n",
    "        states.append((player_sum, dealer_card, 0))  # Hard total\n",
    "\n",
    "# For soft totals (with usable ace), track 12-21\n",
    "# Soft totals below 12 are impossible (A+1 = 12)\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(1, 11):\n",
    "        states.append((player_sum, dealer_card, 1))  # Soft total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Q-table with strategic initial values\n",
    "q_data = {\n",
    "    'State': states,\n",
    "    'Action 0 (Stand)': np.zeros(len(states)),\n",
    "    'Action 1 (Hit)': np.zeros(len(states))\n",
    "}\n",
    "\n",
    "# Strategic initialization: Set high values for \"stand\" in 20-21, high values for \"hit\" in 4-11\n",
    "for i, state in enumerate(states):\n",
    "    player_sum, _, _ = state\n",
    "    if player_sum >= 20:\n",
    "        # For high player sums, initialize stand value higher\n",
    "        q_data['Action 0 (Stand)'][i] = 0.5\n",
    "        q_data['Action 1 (Hit)'][i] = -0.1\n",
    "    elif player_sum < 12:\n",
    "        # For low player sums, initialize hit value higher\n",
    "        q_data['Action 0 (Stand)'][i] = -0.1\n",
    "        q_data['Action 1 (Hit)'][i] = 0.5\n",
    "\n",
    "Q = pl.DataFrame(q_data)\n",
    "\n",
    "# Double Q-learning: Second Q-table for reducing bias\n",
    "Q2 = Q.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ranges = [[0], [1,2,3], [4]]\n",
    "\n",
    "count_data = {\n",
    "    'True Count' : count_ranges,\n",
    "    'Low Bet (1)' : np.zeros(len(count_ranges)),\n",
    "    'Mid Bet (2)' : np.zeros(len(count_ranges)),\n",
    "    'High Bet (5)' : np.zeros(len(count_ranges))\n",
    "}\n",
    "# Initialize betting strategy for each count range, which are fixed in all matches\n",
    "for i, count_range in enumerate(count_ranges):\n",
    "    if count_range == [0]:\n",
    "        count_data['Low Bet (1)'][i] = 0.5\n",
    "        count_data['Mid Bet (2)'][i] = 0.1\n",
    "        count_data['High Bet (5)'][i] = 0.1\n",
    "    elif count_range == [1,2,3]:\n",
    "        count_data['Low Bet (1)'][i] = 0.2\n",
    "        count_data['Mid Bet (2)'][i] = 0.5\n",
    "        count_data['High Bet (5)'][i] = 0.2\n",
    "    else:\n",
    "        count_data['Low Bet (1)'][i] = 0.2\n",
    "        count_data['Mid Bet (2)'][i] = 0.3\n",
    "        count_data['High Bet (5)'][i] = 0.5\n",
    "\n",
    "\n",
    "count_df = pl.DataFrame(count_data)\n",
    "\n",
    "# use a dictionary which associates each column name of the type of bet to the actual bet value\n",
    "betting_strategy = {\n",
    "    'Low Bet (1)': 1,\n",
    "    'Mid Bet (2)': 2,\n",
    "    'High Bet (5)': 5\n",
    "}\n",
    "\n",
    "# Track state-action visit counts for adaptive learning rates\n",
    "visit_counts = {}\n",
    "for state in states:\n",
    "    visit_counts[(state, 0)] = 0  # Stand\n",
    "    visit_counts[(state, 1)] = 0  # Hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Improved hyperparameters\n",
    "initial_lr = 0.1             # Learning rate\n",
    "lr_decay_rate = 0.00005      # Gentler decay rate\n",
    "gamma = 0.95                 # Higher discount factor - long-term rewards matter more\n",
    "n_episodes = 200000         # More training episodes\n",
    "initial_epsilon = 1.0        # Start with 100% exploration\n",
    "epsilon_min = 0.01           # Minimum exploration rate\n",
    "epsilon_decay = 0.99995      # Much slower decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified state representation - focusing on meaningful game states\n",
    "def get_state_features(full_state):\n",
    "    # Extract just player sum, dealer card, and usable ace\n",
    "    return (full_state[0], full_state[1], full_state[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_lr(state, action, base_lr):\n",
    "    \"\"\"Get state-action specific learning rate based on visit count\"\"\"\n",
    "    key = (state, action)\n",
    "    count = visit_counts.get(key, 0) + 1\n",
    "    # Decay learning rate based on visit count, but maintain a minimum rate\n",
    "    return max(base_lr / (1 + 0.005 * count), base_lr * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(state_features, q_table=Q):\n",
    "    \"\"\"Get Q-values for a given state\"\"\"\n",
    "    # Filter the DataFrame for the specific state\n",
    "    state_row = q_table.filter(pl.col('State') == state_features)\n",
    "    \n",
    "    if len(state_row) == 0:\n",
    "        # Return default values based on player sum\n",
    "        player_sum = state_features[0]\n",
    "        if player_sum < 12:\n",
    "            return np.array([-0.1, 0.5])  # Default to hit for low sums\n",
    "        elif player_sum >= 20:\n",
    "            return np.array([0.5, -0.1])  # Default to stand for high sums\n",
    "        else:\n",
    "            return np.array([0.0, 0.0])  # Neutral for middle sums\n",
    "            \n",
    "    # Extract Q-values from the DataFrame\n",
    "    stand_val = state_row.select('Action 0 (Stand)').item()\n",
    "    hit_val = state_row.select('Action 1 (Hit)').item()\n",
    "    return np.array([stand_val, hit_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(state_features, action, reward, next_state_features, lr, q_table=Q, q_table_target=Q2):\n",
    "    \"\"\"Update Q-value for state-action pair using Double Q-learning\"\"\"\n",
    "    # Check if state exists in our table\n",
    "    state_row = q_table.filter(pl.col('State') == state_features)\n",
    "    if len(state_row) == 0:\n",
    "        return  # State not in our table\n",
    "    \n",
    "    # Determine which action column to update\n",
    "    action_col = 'Action 1 (Hit)' if action == 1 else 'Action 0 (Stand)'\n",
    "    \n",
    "    # Current Q-value in the DataFrame\n",
    "    current_q = state_row.select(action_col).item()\n",
    "    \n",
    "    # If next_state_features is None, this is a terminal state\n",
    "    if next_state_features is None:\n",
    "        # Terminal state - no future rewards\n",
    "        new_q = current_q + lr * (reward - current_q)\n",
    "    else:\n",
    "        # Get the next state's best action from current Q-table\n",
    "        next_q_values = get_q_values(next_state_features, q_table)\n",
    "        best_next_action = np.argmax(next_q_values)\n",
    "        \n",
    "        # Get Q-value for best action from target Q-table\n",
    "        next_q_values_target = get_q_values(next_state_features, q_table_target)\n",
    "        max_next_q = next_q_values_target[best_next_action]\n",
    "        \n",
    "        # Q-learning update formula with future rewards\n",
    "        new_q = current_q + lr * (reward + gamma * max_next_q - current_q)\n",
    "    \n",
    "    print(f\"Updating Q-value for state {state_features}, action {action}, reward {reward}, new Q-value: {new_q}\")\n",
    "\n",
    "    # Update the Q-table entry in the DataFrame\n",
    "    # Create a temporary mask for the state we want to update\n",
    "    mask = pl.col('State') == state_features\n",
    "    \n",
    "    # Use the when/then/otherwise pattern to update values\n",
    "    q_table = q_table.with_columns(\n",
    "        pl.when(mask)\n",
    "        .then(pl.lit(new_q))\n",
    "        .otherwise(pl.col(action_col))\n",
    "        .alias(action_col)\n",
    "    )\n",
    "    print(f\"Updated Q-table for state {state_features}: {q_table.filter(mask)}\")\n",
    "    \n",
    "    # Track visit counts\n",
    "    visit_counts[(state_features, action)] = visit_counts.get((state_features, action), 0) + 1\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_count_table(current_count, reward, count_df=count_df):\n",
    "    \"\"\"Update the value associated with the move of betting a certain amount based on the reward received\"\"\"\n",
    "    # Check if reward is positive or negative\n",
    "    if reward > 0:\n",
    "        # Positive reward - increase the value of betting a higher amount and decrease the value of betting a lower amount for the row corresponding to the current count\n",
    "        # Find the index of the current count range\n",
    "        index = None\n",
    "        for i, count_range in enumerate(count_ranges):\n",
    "            if current_count in count_range:\n",
    "                index = i\n",
    "                break\n",
    "        # update the row corresponding to the current count\n",
    "        if index is None:\n",
    "            raise ValueError(\"Current count not found in count ranges\")\n",
    "        # Update the values in the count_df DataFrame\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Low Bet (1)'][index] * 0.9))\n",
    "            .otherwise(pl.col('Low Bet (1)'))\n",
    "            .alias('Low Bet (1)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Mid Bet (2)'][index] * 0.95))\n",
    "            .otherwise(pl.col('Mid Bet (2)'))\n",
    "            .alias('Mid Bet (2)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['High Bet (5)'][index] * 1.05))\n",
    "            .otherwise(pl.col('High Bet (5)'))\n",
    "            .alias('High Bet (5)')\n",
    "        )\n",
    "    elif reward < 0:\n",
    "        # Negative reward - decrease the value of betting a higher amount and increase the value of betting a lower amount for the row corresponding to the current count\n",
    "        # Find the index of the current count range\n",
    "        index = None\n",
    "        for i, count_range in enumerate(count_ranges):\n",
    "            if current_count in count_range:\n",
    "                index = i\n",
    "                break\n",
    "        # update the row corresponding to the current count\n",
    "        if index is None:\n",
    "            raise ValueError(\"Current count not found in count ranges\")\n",
    "        # Update the values in the count_df DataFrame\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Low Bet (1)'][index] * 1.05))\n",
    "            .otherwise(pl.col('Low Bet (1)'))\n",
    "            .alias('Low Bet (1)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['Mid Bet (2)'][index] * 0.95))\n",
    "            .otherwise(pl.col('Mid Bet (2)'))\n",
    "            .alias('Mid Bet (2)')\n",
    "        )\n",
    "        count_df = count_df.with_columns(\n",
    "            pl.when(pl.col('True Count') == count_ranges[index])\n",
    "            .then(pl.lit(count_df['High Bet (5)'][index] * 0.9))\n",
    "            .otherwise(pl.col('High Bet (5)'))\n",
    "            .alias('High Bet (5)')\n",
    "        )\n",
    "    else:\n",
    "        #do nothing\n",
    "        #ALTERNATIVE: update something\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved training...\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (20, 10, 0), action 1, reward -1, new Q-value: -0.1886699507389163\n",
      "Updated Q-table for state (20, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [20, 10, 0] ┆ 0.5              ┆ -0.18867       │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (13, 10, 0), action 0, reward -1, new Q-value: -0.09852216748768475\n",
      "Updated Q-table for state (13, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [13, 10, 0] ┆ -0.098522        ┆ 0.0            │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (20, 10, 0), action 1, reward -1, new Q-value: -0.2682081355219402\n",
      "Updated Q-table for state (20, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [20, 10, 0] ┆ 0.5              ┆ -0.268208      │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (20, 2, 0), action 1, reward -1, new Q-value: -0.18866108463045322\n",
      "Updated Q-table for state (20, 2, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [20, 2, 0] ┆ 0.5              ┆ -0.188661      │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 1\n",
      "Updating Q-value for state (18, 6, 0), action 0, reward 1, new Q-value: 0.09948756442752493\n",
      "Updated Q-table for state (18, 6, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [18, 6, 0] ┆ 0.099488         ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 1\n",
      "Updating Q-value for state (12, 6, 0), action 0, reward 1, new Q-value: 0.09948259104398027\n",
      "Updated Q-table for state (12, 6, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [12, 6, 0] ┆ 0.099483         ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (13, 6, 0), action 0, reward -1, new Q-value: -0.09947761815764965\n",
      "Updated Q-table for state (13, 6, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [13, 6, 0] ┆ -0.099478        ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (13, 10, 0), action 0, reward -1, new Q-value: -0.1868758410436881\n",
      "Updated Q-table for state (13, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [13, 10, 0] ┆ -0.186876        ┆ 0.0            │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (17, 7, 0), action 1, reward -1, new Q-value: -0.09897525964922178\n",
      "Updated Q-table for state (17, 7, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [17, 7, 0] ┆ 0.0              ┆ -0.098975      │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (16, 7, 0), action 0, reward 1, new Q-value: 0.0994627024811966\n",
      "Updated Q-table for state (16, 7, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [16, 7, 0] ┆ 0.099463         ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (18, 10, 0), action 1, reward -1, new Q-value: -0.09751709291794304\n",
      "Updated Q-table for state (18, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [18, 10, 0] ┆ 0.0              ┆ -0.097517      │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (15, 10, 0), action 1, reward -1, new Q-value: -0.09847293102217367\n",
      "Updated Q-table for state (15, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [15, 10, 0] ┆ 0.0              ┆ -0.098473      │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 1\n",
      "Updating Q-value for state (14, 10, 0), action 0, reward -1, new Q-value: -0.09798042742981662\n",
      "Updated Q-table for state (14, 10, 0): shape: (1, 3)\n",
      "┌─────────────┬──────────────────┬────────────────┐\n",
      "│ State       ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---         ┆ ---              ┆ ---            │\n",
      "│ list[i64]   ┆ f64              ┆ f64            │\n",
      "╞═════════════╪══════════════════╪════════════════╡\n",
      "│ [14, 10, 0] ┆ -0.09798         ┆ 0.0            │\n",
      "└─────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (13, 2, 0), action 1, reward 0, new Q-value: 0.0\n",
      "Updated Q-table for state (13, 2, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [13, 2, 0] ┆ 0.0              ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-value for state (19, 2, 0), action 0, reward -1, new Q-value: -0.09894558635896568\n",
      "Updated Q-table for state (19, 2, 0): shape: (1, 3)\n",
      "┌────────────┬──────────────────┬────────────────┐\n",
      "│ State      ┆ Action 0 (Stand) ┆ Action 1 (Hit) │\n",
      "│ ---        ┆ ---              ┆ ---            │\n",
      "│ list[i64]  ┆ f64              ┆ f64            │\n",
      "╞════════════╪══════════════════╪════════════════╡\n",
      "│ [19, 2, 0] ┆ -0.098946        ┆ 0.0            │\n",
      "└────────────┴──────────────────┴────────────────┘\n",
      "Updating Q-table 2\n",
      "Updating Q-table 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating Q-table 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     Q2 \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_q_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbet_amount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Track outcomes\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m, in \u001b[0;36mupdate_q_value\u001b[1;34m(state_features, action, reward, next_state_features, lr, q_table, q_table_target)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update Q-value for state-action pair using Double Q-learning\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check if state exists in our table\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m state_row \u001b[38;5;241m=\u001b[39m \u001b[43mq_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m state_features)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_row) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# State not in our table\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "# Training loop with convergence check\n",
    "print(\"Starting improved training...\")\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "epsilon = initial_epsilon\n",
    "lr = initial_lr\n",
    "money_won = 0\n",
    "money_lost = 0\n",
    "\n",
    "# Parameters for convergence\n",
    "n_episodes = 1000  # Number of episodes for training\n",
    "convergence_threshold = 0.001  # Lower threshold for better stability\n",
    "convergence_check_interval = 10000  # Check for convergence every N episodes\n",
    "convergence_required_count = 3  # Number of consecutive checks below threshold to confirm convergence\n",
    "max_episodes = n_episodes  # Maximum episodes as a fallback\n",
    "\n",
    "# Keep a copy of the previous Q-table for comparison\n",
    "previous_q = Q.clone()\n",
    "convergence_count = 0\n",
    "converged = False\n",
    "episode = 0\n",
    "#first training phase only for the Q-table with fixed betting strategy\n",
    "while episode < max_episodes and not converged:\n",
    "\n",
    "    env.reset()\n",
    "    bet_index = env.bet_space.sample()  # Sample bet index from the environment\n",
    "    bet_amount = env.bets[bet_index]  # Sample bet amount from the environment\n",
    "    state, reward, done, _ = env.step(bet_index, action_type=\"bet\")  # Place bet\n",
    "    # print(bet_amount)\n",
    "    state_features = get_state_features(state)\n",
    "\n",
    "    # Training episode\n",
    "    while not done:\n",
    "        \n",
    "        if state_features[0] < 12:\n",
    "        # Always hit this state as it's not relevant for our training\n",
    "            next_state, _, _, _ = env.step(1, action_type=\"move\")\n",
    "            next_state_features = get_state_features(next_state) if not done else None\n",
    "            state = next_state\n",
    "            state_features = next_state_features if next_state is not None else None\n",
    "            continue\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        elif np.random.rand() < epsilon:\n",
    "            action = env.move_space.sample()  # Random action\n",
    "        else:\n",
    "            q_values = get_q_values(state_features)\n",
    "            action = np.argmax(q_values)  # Greedy action\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, _ = env.step(action, action_type=\"move\")\n",
    "        next_state_features = get_state_features(next_state) if not done else None\n",
    "\n",
    "        # Get adaptive learning rate for this state-action pair\n",
    "        adaptive_lr = get_adaptive_lr(state_features, action, lr)\n",
    "\n",
    "        # Randomly decide which Q-table to update (Double Q-learning)\n",
    "        # print(f\"State: {state_features}, Action: {action}, Done: {done}, Reward: {reward}, Next State: {next_state_features}\")\n",
    "        if np.random.rand() < 0.5:\n",
    "            print(\"Updating Q-table 1\")\n",
    "            Q = update_q_value(state_features, action, reward*bet_amount, next_state_features, adaptive_lr, Q, Q2)\n",
    "        else:\n",
    "            print(\"Updating Q-table 2\")\n",
    "            Q2 = update_q_value(state_features, action, reward*bet_amount, next_state_features, adaptive_lr, Q2, Q)\n",
    "            \n",
    "        # Track outcomes\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                wins += 1\n",
    "                money_won += reward * bet_amount\n",
    "            elif reward == 0:\n",
    "                draws += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "                money_lost += abs(reward) * bet_amount\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = next_state_features if next_state is not None else None\n",
    "        \n",
    "        if state_features is None:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon and learning rate\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    lr = initial_lr / (1 + lr_decay_rate * episode)\n",
    "    \n",
    "    # Check for convergence periodically\n",
    "    if episode % convergence_check_interval == 0 and episode > 0:\n",
    "        # Calculate the maximum absolute difference between current and previous Q-values\n",
    "        diff_stand = (Q.select('Action 0 (Stand)').to_numpy() - \n",
    "                     previous_q.select('Action 0 (Stand)').to_numpy())\n",
    "        diff_hit = (Q.select('Action 1 (Hit)').to_numpy() - \n",
    "                   previous_q.select('Action 1 (Hit)').to_numpy())\n",
    "        \n",
    "        max_diff_stand = np.max(np.abs(diff_stand))\n",
    "        max_diff_hit = np.max(np.abs(diff_hit))\n",
    "        max_diff = max(max_diff_stand, max_diff_hit)\n",
    "        \n",
    "        if max_diff < convergence_threshold:\n",
    "            convergence_count += 1\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f} (convergence count: {convergence_count}/{convergence_required_count})\")\n",
    "            if convergence_count >= convergence_required_count:\n",
    "                print(f\"Converged after {episode} episodes (max Q-value change: {max_diff:.6f})\")\n",
    "                converged = True\n",
    "        else:\n",
    "            convergence_count = 0\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f}\")\n",
    "        \n",
    "        # Store current Q-values for next comparison\n",
    "        previous_q = Q.clone()\n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# Final statistics\n",
    "total_episodes = episode\n",
    "print(f\"Training complete after {total_episodes} episodes.\")\n",
    "print(f\"Win rate: {wins/total_episodes:.4f}\")\n",
    "print(f\"Draw rate: {draws/total_episodes:.4f}\")\n",
    "print(f\"Loss rate: {losses/total_episodes:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final policy evaluation...\n",
      "Final evaluation complete.\n",
      "Win rate: 0.4271\n",
      "Draw rate: 0.0941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the final policy with more episodes\n",
    "print(\"\\nFinal policy evaluation...\")\n",
    "eval_wins = 0\n",
    "eval_draws = 0\n",
    "eval_episodes = 100000\n",
    "\n",
    "for _ in range(eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state_features = get_state_features(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Always choose the best action according to average of both Q-tables\n",
    "        q_values1 = get_q_values(state_features, Q)\n",
    "        q_values2 = get_q_values(state_features, Q2)\n",
    "        avg_q_values = (q_values1 + q_values2) / 2\n",
    "        action = np.argmax(avg_q_values)\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        if done and reward > 0:\n",
    "            eval_wins += 1\n",
    "        elif done and reward == 0:\n",
    "            eval_draws += 1\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = get_state_features(state)\n",
    "\n",
    "print(f\"Final evaluation complete.\")\n",
    "print(f\"Win rate: {eval_wins/eval_episodes:.4f}\")\n",
    "print(f\"Draw rate: {eval_draws/eval_episodes:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Policy (Player Sum vs Dealer Card):\n",
      "Player Sum | Dealer's Card | Usable Ace | Best Action | Q(stand) | Q(hit)\n",
      "---------------------------------------------------------------------------\n",
      "        12 |            1 |          0 | Hit        | -0.6659 | -0.4954\n",
      "        12 |            1 |          1 | Hit        | -0.1234 | -0.1141\n",
      "        12 |            6 |          0 | Stand      | -0.1501 | -0.2494\n",
      "        12 |            6 |          1 | Hit        | -0.0015 |  0.0942\n",
      "        12 |           10 |          0 | Hit        | -0.5323 | -0.4184\n",
      "        12 |           10 |          1 | Hit        | -0.1748 | -0.1282\n",
      "        13 |            1 |          0 | Hit        | -0.7091 | -0.5380\n",
      "        13 |            1 |          1 | Stand      | -0.1076 | -0.1474\n",
      "        13 |            6 |          0 | Stand      | -0.1514 | -0.3095\n",
      "        13 |            6 |          1 | Hit        |  0.0183 |  0.1244\n",
      "        13 |           10 |          0 | Hit        | -0.6062 | -0.4541\n",
      "        13 |           10 |          1 | Hit        | -0.6065 | -0.1466\n",
      "        14 |            1 |          0 | Hit        | -0.7266 | -0.5684\n",
      "        14 |            1 |          1 | Hit        | -0.3345 | -0.2256\n",
      "        14 |            6 |          0 | Stand      | -0.2293 | -0.3084\n",
      "        14 |            6 |          1 | Hit        | -0.0292 |  0.0541\n",
      "        14 |           10 |          0 | Hit        | -0.5865 | -0.4973\n",
      "        14 |           10 |          1 | Hit        | -0.3583 | -0.1927\n",
      "        15 |            1 |          0 | Hit        | -0.7256 | -0.5994\n",
      "        15 |            1 |          1 | Hit        | -0.3042 | -0.2481\n",
      "        15 |            6 |          0 | Stand      | -0.1538 | -0.3871\n",
      "        15 |            6 |          1 | Hit        | -0.1189 |  0.0609\n",
      "        15 |           10 |          0 | Hit        | -0.5933 | -0.5410\n",
      "        15 |           10 |          1 | Hit        | -0.4337 | -0.2221\n",
      "        16 |            1 |          0 | Hit        | -0.6490 | -0.6343\n",
      "        16 |            1 |          1 | Hit        | -0.3216 | -0.3029\n",
      "        16 |            6 |          0 | Stand      | -0.1593 | -0.4821\n",
      "        16 |            6 |          1 | Hit        | -0.0242 |  0.0599\n",
      "        16 |           10 |          0 | Hit        | -0.5875 | -0.5688\n",
      "        16 |           10 |          1 | Hit        | -0.4579 | -0.2610\n",
      "        17 |            1 |          0 | Stand      | -0.6350 | -0.7007\n",
      "        17 |            1 |          1 | Stand      | -0.2535 | -0.3308\n",
      "        17 |            6 |          0 | Stand      |  0.0174 | -0.4623\n",
      "        17 |            6 |          1 | Hit        | -0.1128 |  0.1067\n",
      "        17 |           10 |          0 | Stand      | -0.4694 | -0.5702\n",
      "        17 |           10 |          1 | Hit        | -0.3066 | -0.2381\n",
      "        18 |            1 |          0 | Stand      | -0.3788 | -0.6641\n",
      "        18 |            1 |          1 | Stand      | -0.2547 | -0.2605\n",
      "        18 |            6 |          0 | Stand      |  0.2711 | -0.5797\n",
      "        18 |            6 |          1 | Stand      |  0.2980 |  0.0517\n",
      "        18 |           10 |          0 | Stand      | -0.2392 | -0.6406\n",
      "        18 |           10 |          1 | Hit        | -0.2600 | -0.1891\n",
      "        19 |            1 |          0 | Stand      | -0.1264 | -0.7306\n",
      "        19 |            1 |          1 | Hit        | -0.1090 | -0.0999\n",
      "        19 |            6 |          0 | Stand      |  0.4914 | -0.6951\n",
      "        19 |            6 |          1 | Stand      |  0.4836 |  0.0382\n",
      "        19 |           10 |          0 | Stand      | -0.0216 | -0.7516\n",
      "        19 |           10 |          1 | Stand      | -0.0102 | -0.2280\n",
      "        20 |            1 |          0 | Stand      |  0.1491 | -0.8458\n",
      "        20 |            1 |          1 | Stand      |  0.1670 | -0.0922\n",
      "        20 |            6 |          0 | Stand      |  0.6994 | -0.8514\n",
      "        20 |            6 |          1 | Stand      |  0.6640 | -0.0352\n",
      "        20 |           10 |          0 | Stand      |  0.4399 | -0.8625\n",
      "        20 |           10 |          1 | Stand      |  0.4552 |  0.0061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display policy for critical decision points\n",
    "print(\"\\nLearned Policy (Player Sum vs Dealer Card):\")\n",
    "print(\"Player Sum | Dealer's Card | Usable Ace | Best Action | Q(stand) | Q(hit)\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for player_sum in [12, 13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "    for dealer_card in [1, 6, 10]:  # Dealer showing Ace, 6, 10\n",
    "        for usable_ace in [0, 1]:    # Hard and soft totals\n",
    "            state = (player_sum, dealer_card, usable_ace)\n",
    "            q_values1 = get_q_values(state, Q)\n",
    "            q_values2 = get_q_values(state, Q2)\n",
    "            avg_q_values = (q_values1 + q_values2) / 2\n",
    "            best_action = \"Hit\" if np.argmax(avg_q_values) == 1 else \"Stand\"\n",
    "            print(f\"{player_sum:10d} | {dealer_card:12d} | {usable_ace:10d} | {best_action:10s} | {avg_q_values[0]:7.4f} | {avg_q_values[1]:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average Q-values (ensemble approach)\n",
    "avg_Q = Q.copy()\n",
    "avg_Q['Action 0 (Stand)'] = (Q['Action 0 (Stand)'] + Q2['Action 0 (Stand)']) / 2\n",
    "avg_Q['Action 1 (Hit)'] = (Q['Action 1 (Hit)'] + Q2['Action 1 (Hit)']) / 2\n",
    "avg_Q['Best Action'] = avg_Q.apply(\n",
    "    lambda row: \"Stand\" if row['Action 0 (Stand)'] > row['Action 1 (Hit)'] else \"Hit\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.to_csv('blackjack_q_table1_cpu.csv', index=False)\n",
    "Q2.to_csv('blackjack_q_table2_cpu.csv', index=False)\n",
    "avg_Q.to_csv('blackjack_avg_q_table_cpu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
