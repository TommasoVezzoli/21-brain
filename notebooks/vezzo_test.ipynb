{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multi_deck_env import BlackjackMultiDeckEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create environment with 6 decks (standard casino configuration)\n",
    "env = BlackjackMultiDeckEnv(num_decks=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified state representation - focusing on meaningful game states\n",
    "def get_state_features(full_state):\n",
    "    # Extract just player sum, dealer card, and usable ace\n",
    "    return (full_state[0], full_state[1], full_state[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate relevant states - focusing on decision points that matter\n",
    "states = []\n",
    "# For hard totals (no usable ace), only track 12-21\n",
    "# Below 12, the optimal play is always hit\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(1, 11):\n",
    "        states.append((player_sum, dealer_card, 0))  # Hard total\n",
    "\n",
    "# For soft totals (with usable ace), track 12-21\n",
    "# Soft totals below 12 are impossible (A+1 = 12)\n",
    "for player_sum in range(12, 22):\n",
    "    for dealer_card in range(1, 11):\n",
    "        states.append((player_sum, dealer_card, 1))  # Soft total\n",
    "\n",
    "# For completeness, include states 4-11 which always hit\n",
    "for player_sum in range(4, 12):\n",
    "    for dealer_card in range(1, 11):\n",
    "        for usable_ace in [0, 1]:\n",
    "            # Only add if it's a valid state (some combinations aren't possible)\n",
    "            if not (usable_ace == 1 and player_sum < 12):\n",
    "                states.append((player_sum, dealer_card, usable_ace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Q-table with strategic initial values\n",
    "q_data = {\n",
    "    'State': states,\n",
    "    'Action 0 (Stand)': np.zeros(len(states)),\n",
    "    'Action 1 (Hit)': np.zeros(len(states))\n",
    "}\n",
    "\n",
    "# Strategic initialization: Set high values for \"stand\" in 20-21, high values for \"hit\" in 4-11\n",
    "for i, state in enumerate(states):\n",
    "    player_sum, _, _ = state\n",
    "    if player_sum >= 20:\n",
    "        # For high player sums, initialize stand value higher\n",
    "        q_data['Action 0 (Stand)'][i] = 0.5\n",
    "        q_data['Action 1 (Hit)'][i] = -0.1\n",
    "    elif player_sum < 12:\n",
    "        # For low player sums, initialize hit value higher\n",
    "        q_data['Action 0 (Stand)'][i] = -0.1\n",
    "        q_data['Action 1 (Hit)'][i] = 0.5\n",
    "\n",
    "Q = pd.DataFrame(q_data)\n",
    "\n",
    "# Double Q-learning: Second Q-table for reducing bias\n",
    "Q2 = Q.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Improved hyperparameters\n",
    "initial_lr = 0.1             # Learning rate\n",
    "lr_decay_rate = 0.00005      # Gentler decay rate\n",
    "gamma = 0.95                 # Higher discount factor - long-term rewards matter more\n",
    "n_episodes = 1000000         # More training episodes\n",
    "initial_epsilon = 1.0        # Start with 100% exploration\n",
    "epsilon_min = 0.01           # Minimum exploration rate\n",
    "epsilon_decay = 0.99995      # Much slower decay rate\n",
    "\n",
    "# Track state-action visit counts for adaptive learning rates\n",
    "visit_counts = {}\n",
    "for state in states:\n",
    "    visit_counts[(state, 0)] = 0  # Stand\n",
    "    visit_counts[(state, 1)] = 0  # Hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_adaptive_lr(state, action, base_lr):\n",
    "    \"\"\"Get state-action specific learning rate based on visit count\"\"\"\n",
    "    key = (state, action)\n",
    "    count = visit_counts.get(key, 0) + 1\n",
    "    # Decay learning rate based on visit count, but maintain a minimum rate\n",
    "    return max(base_lr / (1 + 0.005 * count), base_lr * 0.1)\n",
    "\n",
    "def get_q_values(state_features, q_table=Q):\n",
    "    \"\"\"Get Q-values for a given state\"\"\"\n",
    "    state_row = q_table[q_table['State'] == state_features]\n",
    "    if len(state_row) == 0:\n",
    "        # Return default values based on player sum\n",
    "        player_sum = state_features[0]\n",
    "        if player_sum < 12:\n",
    "            return [-0.1, 0.5]  # Default to hit for low sums\n",
    "        elif player_sum >= 20:\n",
    "            return [0.5, -0.1]  # Default to stand for high sums\n",
    "        else:\n",
    "            return [0.0, 0.0]  # Neutral for middle sums\n",
    "    return state_row[['Action 0 (Stand)', 'Action 1 (Hit)']].values[0]\n",
    "\n",
    "def update_q_value(state_features, action, reward, next_state_features, lr, q_table=Q, q_table_target=Q2):\n",
    "    \"\"\"Update Q-value for state-action pair using Double Q-learning\"\"\"\n",
    "    # Find the row for the current state\n",
    "    state_idx = q_table.index[q_table['State'] == state_features].tolist()\n",
    "    if not state_idx:\n",
    "        return  # State not in our table\n",
    "    \n",
    "    # Determine which action column to update\n",
    "    action_col = 'Action 1 (Hit)' if action == 1 else 'Action 0 (Stand)'\n",
    "    \n",
    "    # Current Q-value\n",
    "    current_q = q_table.loc[state_idx[0], action_col]\n",
    "    \n",
    "    # If next_state_features is None, this is a terminal state\n",
    "    if next_state_features is None:\n",
    "        # Terminal state - no future rewards\n",
    "        new_q = current_q + lr * (reward - current_q)\n",
    "    else:\n",
    "        # Get the next state's best action from current Q-table\n",
    "        next_q_values = get_q_values(next_state_features, q_table)\n",
    "        best_next_action = np.argmax(next_q_values)\n",
    "        \n",
    "        # Get Q-value for best action from target Q-table\n",
    "        next_q_values_target = get_q_values(next_state_features, q_table_target)\n",
    "        max_next_q = next_q_values_target[best_next_action]\n",
    "        \n",
    "        # Q-learning update formula with future rewards\n",
    "        new_q = current_q + lr * (reward + gamma * max_next_q - current_q)\n",
    "    \n",
    "    # Update the Q-table\n",
    "    q_table.loc[state_idx[0], action_col] = new_q\n",
    "    \n",
    "    # Track visit counts\n",
    "    visit_counts[(state_features, action)] = visit_counts.get((state_features, action), 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved training...\n",
      "Episode 0, epsilon: 1.0000, lr: 0.100000, wins: 0, draws: 0, losses: 0\n",
      "Episode 10000, epsilon: 0.6065, lr: 0.066669, wins: 3061, draws: 482, losses: 6457\n",
      "Episode 10000, max Q-value change: 0.832371\n",
      "Episode 20000, epsilon: 0.3679, lr: 0.050001, wins: 6584, draws: 1139, losses: 12277\n",
      "Episode 20000, max Q-value change: 0.463592\n",
      "Episode 30000, epsilon: 0.2231, lr: 0.040001, wins: 10350, draws: 1876, losses: 17774\n",
      "Episode 30000, max Q-value change: 0.276282\n",
      "Episode 40000, epsilon: 0.1353, lr: 0.033334, wins: 14239, draws: 2731, losses: 23030\n",
      "Episode 40000, max Q-value change: 0.219286\n",
      "Episode 50000, epsilon: 0.0821, lr: 0.028572, wins: 18337, draws: 3627, losses: 28036\n",
      "EVALUATION: Win rate: 0.4267, Draw rate: 0.0969\n",
      "Episode 50000, max Q-value change: 0.211416\n",
      "Episode 60000, epsilon: 0.0498, lr: 0.025000, wins: 22496, draws: 4523, losses: 32981\n",
      "Episode 60000, max Q-value change: 0.193939\n",
      "Episode 70000, epsilon: 0.0302, lr: 0.022222, wins: 26623, draws: 5433, losses: 37944\n",
      "Episode 70000, max Q-value change: 0.112994\n",
      "Episode 80000, epsilon: 0.0183, lr: 0.020000, wins: 30801, draws: 6382, losses: 42817\n",
      "Episode 80000, max Q-value change: 0.102630\n",
      "Episode 90000, epsilon: 0.0111, lr: 0.018182, wins: 35096, draws: 7280, losses: 47624\n",
      "Episode 90000, max Q-value change: 0.151521\n",
      "Episode 100000, epsilon: 0.0100, lr: 0.016667, wins: 39314, draws: 8174, losses: 52512\n",
      "EVALUATION: Win rate: 0.4281, Draw rate: 0.0909\n",
      "Episode 100000, max Q-value change: 0.044292\n",
      "Episode 110000, epsilon: 0.0100, lr: 0.015385, wins: 43542, draws: 9145, losses: 57313\n",
      "Episode 110000, max Q-value change: 0.060099\n",
      "Episode 120000, epsilon: 0.0100, lr: 0.014286, wins: 47809, draws: 10176, losses: 62015\n",
      "Episode 120000, max Q-value change: 0.054448\n",
      "Episode 130000, epsilon: 0.0100, lr: 0.013333, wins: 52044, draws: 11112, losses: 66844\n",
      "Episode 130000, max Q-value change: 0.041403\n",
      "Episode 140000, epsilon: 0.0100, lr: 0.012500, wins: 56217, draws: 12046, losses: 71737\n",
      "Episode 140000, max Q-value change: 0.037951\n",
      "Episode 150000, epsilon: 0.0100, lr: 0.011765, wins: 60541, draws: 12959, losses: 76500\n",
      "EVALUATION: Win rate: 0.4292, Draw rate: 0.0951\n",
      "Episode 150000, max Q-value change: 0.041851\n",
      "Episode 160000, epsilon: 0.0100, lr: 0.011111, wins: 64753, draws: 13884, losses: 81363\n",
      "Episode 160000, max Q-value change: 0.029967\n",
      "Episode 170000, epsilon: 0.0100, lr: 0.010526, wins: 69092, draws: 14825, losses: 86083\n",
      "Episode 170000, max Q-value change: 0.025977\n",
      "Episode 180000, epsilon: 0.0100, lr: 0.010000, wins: 73333, draws: 15726, losses: 90941\n",
      "Episode 180000, max Q-value change: 0.026419\n",
      "Episode 190000, epsilon: 0.0100, lr: 0.009524, wins: 77597, draws: 16630, losses: 95773\n",
      "Episode 190000, max Q-value change: 0.026106\n",
      "Episode 200000, epsilon: 0.0100, lr: 0.009091, wins: 81851, draws: 17590, losses: 100559\n",
      "EVALUATION: Win rate: 0.4174, Draw rate: 0.0936\n",
      "Episode 200000, max Q-value change: 0.020368\n",
      "Episode 210000, epsilon: 0.0100, lr: 0.008696, wins: 86065, draws: 18547, losses: 105388\n",
      "Episode 210000, max Q-value change: 0.020432\n",
      "Episode 220000, epsilon: 0.0100, lr: 0.008333, wins: 90331, draws: 19483, losses: 110186\n",
      "Episode 220000, max Q-value change: 0.016292\n",
      "Episode 230000, epsilon: 0.0100, lr: 0.008000, wins: 94573, draws: 20456, losses: 114971\n",
      "Episode 230000, max Q-value change: 0.016471\n",
      "Episode 240000, epsilon: 0.0100, lr: 0.007692, wins: 98813, draws: 21431, losses: 119756\n",
      "Episode 240000, max Q-value change: 0.016506\n",
      "Episode 250000, epsilon: 0.0100, lr: 0.007407, wins: 103086, draws: 22378, losses: 124536\n",
      "EVALUATION: Win rate: 0.4218, Draw rate: 0.0945\n",
      "Episode 250000, max Q-value change: 0.018301\n",
      "Episode 260000, epsilon: 0.0100, lr: 0.007143, wins: 107355, draws: 23336, losses: 129309\n",
      "Episode 260000, max Q-value change: 0.021372\n",
      "Episode 270000, epsilon: 0.0100, lr: 0.006897, wins: 111578, draws: 24275, losses: 134147\n",
      "Episode 270000, max Q-value change: 0.013309\n",
      "Episode 280000, epsilon: 0.0100, lr: 0.006667, wins: 115837, draws: 25218, losses: 138945\n",
      "Episode 280000, max Q-value change: 0.018758\n",
      "Episode 290000, epsilon: 0.0100, lr: 0.006452, wins: 120082, draws: 26154, losses: 143764\n",
      "Episode 290000, max Q-value change: 0.013834\n",
      "Episode 300000, epsilon: 0.0100, lr: 0.006250, wins: 124374, draws: 27037, losses: 148589\n",
      "EVALUATION: Win rate: 0.4240, Draw rate: 0.0923\n",
      "Episode 300000, max Q-value change: 0.010590\n",
      "Episode 310000, epsilon: 0.0100, lr: 0.006061, wins: 128657, draws: 27964, losses: 153379\n",
      "Episode 310000, max Q-value change: 0.009968\n",
      "Episode 320000, epsilon: 0.0100, lr: 0.005882, wins: 132885, draws: 28885, losses: 158230\n",
      "Episode 320000, max Q-value change: 0.013961\n",
      "Episode 330000, epsilon: 0.0100, lr: 0.005714, wins: 137193, draws: 29803, losses: 163004\n",
      "Episode 330000, max Q-value change: 0.011056\n",
      "Episode 340000, epsilon: 0.0100, lr: 0.005556, wins: 141424, draws: 30751, losses: 167825\n",
      "Episode 340000, max Q-value change: 0.015137\n",
      "Episode 350000, epsilon: 0.0100, lr: 0.005405, wins: 145675, draws: 31664, losses: 172661\n",
      "EVALUATION: Win rate: 0.4210, Draw rate: 0.0930\n",
      "Episode 350000, max Q-value change: 0.009685\n",
      "Episode 360000, epsilon: 0.0100, lr: 0.005263, wins: 149893, draws: 32661, losses: 177446\n",
      "Episode 360000, max Q-value change: 0.009990\n",
      "Episode 370000, epsilon: 0.0100, lr: 0.005128, wins: 154219, draws: 33541, losses: 182240\n",
      "Episode 370000, max Q-value change: 0.006771\n",
      "Episode 380000, epsilon: 0.0100, lr: 0.005000, wins: 158450, draws: 34460, losses: 187090\n",
      "Episode 380000, max Q-value change: 0.013387\n",
      "Episode 390000, epsilon: 0.0100, lr: 0.004878, wins: 162721, draws: 35419, losses: 191860\n",
      "Episode 390000, max Q-value change: 0.012177\n",
      "Episode 400000, epsilon: 0.0100, lr: 0.004762, wins: 166980, draws: 36380, losses: 196640\n",
      "EVALUATION: Win rate: 0.4273, Draw rate: 0.0920\n",
      "Episode 400000, max Q-value change: 0.008067\n",
      "Episode 410000, epsilon: 0.0100, lr: 0.004651, wins: 171242, draws: 37306, losses: 201452\n",
      "Episode 410000, max Q-value change: 0.011751\n",
      "Episode 420000, epsilon: 0.0100, lr: 0.004545, wins: 175473, draws: 38217, losses: 206310\n",
      "Episode 420000, max Q-value change: 0.009949\n",
      "Episode 430000, epsilon: 0.0100, lr: 0.004444, wins: 179755, draws: 39165, losses: 211080\n",
      "Episode 430000, max Q-value change: 0.006981\n",
      "Episode 440000, epsilon: 0.0100, lr: 0.004348, wins: 184019, draws: 40139, losses: 215842\n",
      "Episode 440000, max Q-value change: 0.008272\n",
      "Episode 450000, epsilon: 0.0100, lr: 0.004255, wins: 188326, draws: 41095, losses: 220579\n",
      "EVALUATION: Win rate: 0.4326, Draw rate: 0.0967\n",
      "Episode 450000, max Q-value change: 0.011927\n",
      "Episode 460000, epsilon: 0.0100, lr: 0.004167, wins: 192623, draws: 42070, losses: 225307\n",
      "Episode 460000, max Q-value change: 0.009529\n",
      "Episode 470000, epsilon: 0.0100, lr: 0.004082, wins: 196890, draws: 43006, losses: 230104\n",
      "Episode 470000, max Q-value change: 0.009972\n",
      "Episode 480000, epsilon: 0.0100, lr: 0.004000, wins: 201099, draws: 43877, losses: 235024\n",
      "Episode 480000, max Q-value change: 0.010612\n",
      "Episode 490000, epsilon: 0.0100, lr: 0.003922, wins: 205348, draws: 44832, losses: 239820\n",
      "Episode 490000, max Q-value change: 0.016527\n",
      "Episode 500000, epsilon: 0.0100, lr: 0.003846, wins: 209605, draws: 45752, losses: 244643\n",
      "EVALUATION: Win rate: 0.4311, Draw rate: 0.0945\n",
      "Episode 500000, max Q-value change: 0.007253\n",
      "Episode 510000, epsilon: 0.0100, lr: 0.003774, wins: 213958, draws: 46684, losses: 249358\n",
      "Episode 510000, max Q-value change: 0.011631\n",
      "Episode 520000, epsilon: 0.0100, lr: 0.003704, wins: 218166, draws: 47611, losses: 254223\n",
      "Episode 520000, max Q-value change: 0.006015\n",
      "Episode 530000, epsilon: 0.0100, lr: 0.003636, wins: 222478, draws: 48540, losses: 258982\n",
      "Episode 530000, max Q-value change: 0.006712\n",
      "Episode 540000, epsilon: 0.0100, lr: 0.003571, wins: 226723, draws: 49482, losses: 263795\n",
      "Episode 540000, max Q-value change: 0.006630\n",
      "Episode 550000, epsilon: 0.0100, lr: 0.003509, wins: 230955, draws: 50463, losses: 268582\n",
      "EVALUATION: Win rate: 0.4336, Draw rate: 0.0927\n",
      "Episode 550000, max Q-value change: 0.005253\n",
      "Episode 560000, epsilon: 0.0100, lr: 0.003448, wins: 235298, draws: 51391, losses: 273311\n",
      "Episode 560000, max Q-value change: 0.005641\n",
      "Episode 570000, epsilon: 0.0100, lr: 0.003390, wins: 239502, draws: 52320, losses: 278178\n",
      "Episode 570000, max Q-value change: 0.005722\n",
      "Episode 580000, epsilon: 0.0100, lr: 0.003333, wins: 243747, draws: 53220, losses: 283033\n",
      "Episode 580000, max Q-value change: 0.007820\n",
      "Episode 590000, epsilon: 0.0100, lr: 0.003279, wins: 247967, draws: 54140, losses: 287893\n",
      "Episode 590000, max Q-value change: 0.008341\n",
      "Episode 600000, epsilon: 0.0100, lr: 0.003226, wins: 252202, draws: 55020, losses: 292778\n",
      "EVALUATION: Win rate: 0.4317, Draw rate: 0.0950\n",
      "Episode 600000, max Q-value change: 0.006461\n",
      "Episode 610000, epsilon: 0.0100, lr: 0.003175, wins: 256462, draws: 55944, losses: 297594\n",
      "Episode 610000, max Q-value change: 0.004807\n",
      "Episode 620000, epsilon: 0.0100, lr: 0.003125, wins: 260685, draws: 56875, losses: 302440\n",
      "Episode 620000, max Q-value change: 0.006180\n",
      "Episode 630000, epsilon: 0.0100, lr: 0.003077, wins: 264987, draws: 57816, losses: 307197\n",
      "Episode 630000, max Q-value change: 0.008781\n",
      "Episode 640000, epsilon: 0.0100, lr: 0.003030, wins: 269286, draws: 58744, losses: 311970\n",
      "Episode 640000, max Q-value change: 0.005933\n",
      "Episode 650000, epsilon: 0.0100, lr: 0.002985, wins: 273570, draws: 59722, losses: 316708\n",
      "EVALUATION: Win rate: 0.4247, Draw rate: 0.0917\n",
      "Episode 650000, max Q-value change: 0.004280\n",
      "Episode 660000, epsilon: 0.0100, lr: 0.002941, wins: 277735, draws: 60686, losses: 321579\n",
      "Episode 660000, max Q-value change: 0.009784\n",
      "Episode 670000, epsilon: 0.0100, lr: 0.002899, wins: 281959, draws: 61644, losses: 326397\n",
      "Episode 670000, max Q-value change: 0.005802\n",
      "Episode 680000, epsilon: 0.0100, lr: 0.002857, wins: 286220, draws: 62648, losses: 331132\n",
      "Episode 680000, max Q-value change: 0.005550\n",
      "Episode 690000, epsilon: 0.0100, lr: 0.002817, wins: 290470, draws: 63579, losses: 335951\n",
      "Episode 690000, max Q-value change: 0.005237\n",
      "Episode 700000, epsilon: 0.0100, lr: 0.002778, wins: 294720, draws: 64525, losses: 340755\n",
      "EVALUATION: Win rate: 0.4399, Draw rate: 0.0950\n",
      "Episode 700000, max Q-value change: 0.006176\n",
      "Episode 710000, epsilon: 0.0100, lr: 0.002740, wins: 298948, draws: 65442, losses: 345610\n",
      "Episode 710000, max Q-value change: 0.004376\n",
      "Episode 720000, epsilon: 0.0100, lr: 0.002703, wins: 303163, draws: 66376, losses: 350461\n",
      "Episode 720000, max Q-value change: 0.003762\n",
      "Episode 730000, epsilon: 0.0100, lr: 0.002667, wins: 307371, draws: 67295, losses: 355334\n",
      "Episode 730000, max Q-value change: 0.004681\n",
      "Episode 740000, epsilon: 0.0100, lr: 0.002632, wins: 311664, draws: 68150, losses: 360186\n",
      "Episode 740000, max Q-value change: 0.003343\n",
      "Episode 750000, epsilon: 0.0100, lr: 0.002597, wins: 315990, draws: 69066, losses: 364944\n",
      "EVALUATION: Win rate: 0.4256, Draw rate: 0.0930\n",
      "Episode 750000, max Q-value change: 0.004093\n",
      "Episode 760000, epsilon: 0.0100, lr: 0.002564, wins: 320316, draws: 69976, losses: 369708\n",
      "Episode 760000, max Q-value change: 0.005244\n",
      "Episode 770000, epsilon: 0.0100, lr: 0.002532, wins: 324564, draws: 70961, losses: 374475\n",
      "Episode 770000, max Q-value change: 0.004432\n",
      "Episode 780000, epsilon: 0.0100, lr: 0.002500, wins: 328819, draws: 71872, losses: 379309\n",
      "Episode 780000, max Q-value change: 0.004354\n",
      "Episode 790000, epsilon: 0.0100, lr: 0.002469, wins: 333101, draws: 72825, losses: 384074\n",
      "Episode 790000, max Q-value change: 0.004981\n",
      "Episode 800000, epsilon: 0.0100, lr: 0.002439, wins: 337377, draws: 73786, losses: 388837\n",
      "EVALUATION: Win rate: 0.4304, Draw rate: 0.0984\n",
      "Episode 800000, max Q-value change: 0.004116\n",
      "Episode 810000, epsilon: 0.0100, lr: 0.002410, wins: 341685, draws: 74688, losses: 393627\n",
      "Episode 810000, max Q-value change: 0.003702\n",
      "Episode 820000, epsilon: 0.0100, lr: 0.002381, wins: 346071, draws: 75604, losses: 398325\n",
      "Episode 820000, max Q-value change: 0.004037\n",
      "Episode 830000, epsilon: 0.0100, lr: 0.002353, wins: 350234, draws: 76548, losses: 403218\n",
      "Episode 830000, max Q-value change: 0.005250\n",
      "Episode 840000, epsilon: 0.0100, lr: 0.002326, wins: 354458, draws: 77488, losses: 408054\n",
      "Episode 840000, max Q-value change: 0.005148\n",
      "Episode 850000, epsilon: 0.0100, lr: 0.002299, wins: 358786, draws: 78463, losses: 412751\n",
      "EVALUATION: Win rate: 0.4193, Draw rate: 0.0935\n",
      "Episode 850000, max Q-value change: 0.003595\n",
      "Episode 860000, epsilon: 0.0100, lr: 0.002273, wins: 363067, draws: 79378, losses: 417555\n",
      "Episode 860000, max Q-value change: 0.004100\n",
      "Episode 870000, epsilon: 0.0100, lr: 0.002247, wins: 367234, draws: 80337, losses: 422429\n",
      "Episode 870000, max Q-value change: 0.007202\n",
      "Episode 880000, epsilon: 0.0100, lr: 0.002222, wins: 371469, draws: 81265, losses: 427266\n",
      "Episode 880000, max Q-value change: 0.003368\n",
      "Episode 890000, epsilon: 0.0100, lr: 0.002198, wins: 375764, draws: 82235, losses: 432001\n",
      "Episode 890000, max Q-value change: 0.004114\n",
      "Episode 900000, epsilon: 0.0100, lr: 0.002174, wins: 380036, draws: 83201, losses: 436763\n",
      "EVALUATION: Win rate: 0.4236, Draw rate: 0.0927\n",
      "Episode 900000, max Q-value change: 0.004148\n",
      "Episode 910000, epsilon: 0.0100, lr: 0.002151, wins: 384313, draws: 84137, losses: 441550\n",
      "Episode 910000, max Q-value change: 0.003572\n",
      "Episode 920000, epsilon: 0.0100, lr: 0.002128, wins: 388576, draws: 85063, losses: 446361\n",
      "Episode 920000, max Q-value change: 0.003066\n",
      "Episode 930000, epsilon: 0.0100, lr: 0.002105, wins: 392863, draws: 86022, losses: 451115\n",
      "Episode 930000, max Q-value change: 0.005061\n",
      "Episode 940000, epsilon: 0.0100, lr: 0.002083, wins: 397155, draws: 86991, losses: 455854\n",
      "Episode 940000, max Q-value change: 0.003424\n",
      "Episode 950000, epsilon: 0.0100, lr: 0.002062, wins: 401406, draws: 87943, losses: 460651\n",
      "EVALUATION: Win rate: 0.4179, Draw rate: 0.0965\n",
      "Episode 950000, max Q-value change: 0.003920\n",
      "Episode 960000, epsilon: 0.0100, lr: 0.002041, wins: 405688, draws: 88881, losses: 465431\n",
      "Episode 960000, max Q-value change: 0.003603\n",
      "Episode 970000, epsilon: 0.0100, lr: 0.002020, wins: 409922, draws: 89843, losses: 470235\n",
      "Episode 970000, max Q-value change: 0.004875\n",
      "Episode 980000, epsilon: 0.0100, lr: 0.002000, wins: 414134, draws: 90848, losses: 475018\n",
      "Episode 980000, max Q-value change: 0.007593\n",
      "Episode 990000, epsilon: 0.0100, lr: 0.001980, wins: 418339, draws: 91783, losses: 479878\n",
      "Episode 990000, max Q-value change: 0.003951\n",
      "Training complete after 1000000 episodes.\n",
      "Win rate: 0.4226\n",
      "Draw rate: 0.0927\n",
      "Loss rate: 0.4847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop with convergence check\n",
    "print(\"Starting improved training...\")\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "epsilon = initial_epsilon\n",
    "lr = initial_lr\n",
    "\n",
    "# Parameters for convergence\n",
    "convergence_threshold = 0.0005  # Lower threshold for better stability\n",
    "convergence_check_interval = 10000  # Check for convergence every N episodes\n",
    "convergence_required_count = 3  # Number of consecutive checks below threshold to confirm convergence\n",
    "max_episodes = n_episodes  # Maximum episodes as a fallback\n",
    "\n",
    "# Keep a copy of the previous Q-table for comparison\n",
    "previous_q = Q.copy()\n",
    "convergence_count = 0\n",
    "converged = False\n",
    "episode = 0\n",
    "\n",
    "# Add simple evaluation every 50k episodes\n",
    "eval_intervals = 50000\n",
    "last_win_rate = 0\n",
    "\n",
    "while episode < max_episodes and not converged:\n",
    "    state, _ = env.reset()\n",
    "    state_features = get_state_features(state)\n",
    "    done = False\n",
    "    \n",
    "    # Progress reporting\n",
    "    if episode % 10000 == 0:\n",
    "        print(f\"Episode {episode}, epsilon: {epsilon:.4f}, lr: {lr:.6f}, wins: {wins}, draws: {draws}, losses: {losses}\")\n",
    "    \n",
    "    # Run quick evaluation every eval_intervals episodes\n",
    "    if episode % eval_intervals == 0 and episode > 0:\n",
    "        eval_wins = 0\n",
    "        eval_draws = 0\n",
    "        eval_episodes = 10000\n",
    "        \n",
    "        for _ in range(eval_episodes):\n",
    "            eval_state, _ = env.reset()\n",
    "            eval_state_features = get_state_features(eval_state)\n",
    "            eval_done = False\n",
    "            \n",
    "            while not eval_done:\n",
    "                # Always choose the best action\n",
    "                q_values = get_q_values(eval_state_features)\n",
    "                eval_action = np.argmax(q_values)\n",
    "                eval_next_state, eval_reward, eval_done, _, _ = env.step(eval_action)\n",
    "                \n",
    "                if eval_done and eval_reward > 0:\n",
    "                    eval_wins += 1\n",
    "                elif eval_done and eval_reward == 0:\n",
    "                    eval_draws += 1\n",
    "                \n",
    "                eval_state = eval_next_state\n",
    "                eval_state_features = get_state_features(eval_state)\n",
    "        \n",
    "        win_rate = eval_wins / eval_episodes\n",
    "        draw_rate = eval_draws / eval_episodes\n",
    "        print(f\"EVALUATION: Win rate: {win_rate:.4f}, Draw rate: {draw_rate:.4f}\")\n",
    "        \n",
    "        # Check if win rate improvement has plateaued\n",
    "        if abs(win_rate - last_win_rate) < 0.001 and episode > 300000:\n",
    "            print(f\"Win rate improvement has plateaued at {win_rate:.4f}\")\n",
    "            # Optionally break if no improvement\n",
    "            # converged = True\n",
    "        \n",
    "        last_win_rate = win_rate\n",
    "    \n",
    "    # Training episode\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            q_values = get_q_values(state_features)\n",
    "            action = np.argmax(q_values)  # Greedy action\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_features = get_state_features(next_state) if not done else None\n",
    "\n",
    "        # Get adaptive learning rate for this state-action pair\n",
    "        adaptive_lr = get_adaptive_lr(state_features, action, lr)\n",
    "\n",
    "        # Randomly decide which Q-table to update (Double Q-learning)\n",
    "        if np.random.rand() < 0.5:\n",
    "            update_q_value(state_features, action, reward, next_state_features, adaptive_lr, Q, Q2)\n",
    "        else:\n",
    "            update_q_value(state_features, action, reward, next_state_features, adaptive_lr, Q2, Q)\n",
    "        # Track outcomes\n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                wins += 1\n",
    "            elif reward == 0:\n",
    "                draws += 1\n",
    "            else:\n",
    "                losses += 1\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = next_state_features if next_state is not None else None\n",
    "    \n",
    "    # Decay epsilon and learning rate\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    lr = initial_lr / (1 + lr_decay_rate * episode)\n",
    "    \n",
    "    # Check for convergence periodically\n",
    "    if episode % convergence_check_interval == 0 and episode > 0:\n",
    "        # Calculate the maximum absolute difference between current and previous Q-values\n",
    "        max_diff_stand = np.max(np.abs(Q['Action 0 (Stand)'] - previous_q['Action 0 (Stand)']))\n",
    "        max_diff_hit = np.max(np.abs(Q['Action 1 (Hit)'] - previous_q['Action 1 (Hit)']))\n",
    "        max_diff = max(max_diff_stand, max_diff_hit)\n",
    "        \n",
    "        if max_diff < convergence_threshold:\n",
    "            convergence_count += 1\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f} (convergence count: {convergence_count}/{convergence_required_count})\")\n",
    "            if convergence_count >= convergence_required_count:\n",
    "                print(f\"Converged after {episode} episodes (max Q-value change: {max_diff:.6f})\")\n",
    "                converged = True\n",
    "        else:\n",
    "            convergence_count = 0\n",
    "            print(f\"Episode {episode}, max Q-value change: {max_diff:.6f}\")\n",
    "        \n",
    "        # Store current Q-values for next comparison\n",
    "        previous_q = Q.copy()\n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "# Final statistics\n",
    "total_episodes = episode\n",
    "print(f\"Training complete after {total_episodes} episodes.\")\n",
    "print(f\"Win rate: {wins/total_episodes:.4f}\")\n",
    "print(f\"Draw rate: {draws/total_episodes:.4f}\")\n",
    "print(f\"Loss rate: {losses/total_episodes:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final policy evaluation...\n",
      "Final evaluation complete.\n",
      "Win rate: 0.4271\n",
      "Draw rate: 0.0941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the final policy with more episodes\n",
    "print(\"\\nFinal policy evaluation...\")\n",
    "eval_wins = 0\n",
    "eval_draws = 0\n",
    "eval_episodes = 100000\n",
    "\n",
    "for _ in range(eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state_features = get_state_features(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Always choose the best action according to average of both Q-tables\n",
    "        q_values1 = get_q_values(state_features, Q)\n",
    "        q_values2 = get_q_values(state_features, Q2)\n",
    "        avg_q_values = (q_values1 + q_values2) / 2\n",
    "        action = np.argmax(avg_q_values)\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        if done and reward > 0:\n",
    "            eval_wins += 1\n",
    "        elif done and reward == 0:\n",
    "            eval_draws += 1\n",
    "        \n",
    "        state = next_state\n",
    "        state_features = get_state_features(state)\n",
    "\n",
    "print(f\"Final evaluation complete.\")\n",
    "print(f\"Win rate: {eval_wins/eval_episodes:.4f}\")\n",
    "print(f\"Draw rate: {eval_draws/eval_episodes:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Policy (Player Sum vs Dealer Card):\n",
      "Player Sum | Dealer's Card | Usable Ace | Best Action | Q(stand) | Q(hit)\n",
      "---------------------------------------------------------------------------\n",
      "        12 |            1 |          0 | Hit        | -0.6659 | -0.4954\n",
      "        12 |            1 |          1 | Hit        | -0.1234 | -0.1141\n",
      "        12 |            6 |          0 | Stand      | -0.1501 | -0.2494\n",
      "        12 |            6 |          1 | Hit        | -0.0015 |  0.0942\n",
      "        12 |           10 |          0 | Hit        | -0.5323 | -0.4184\n",
      "        12 |           10 |          1 | Hit        | -0.1748 | -0.1282\n",
      "        13 |            1 |          0 | Hit        | -0.7091 | -0.5380\n",
      "        13 |            1 |          1 | Stand      | -0.1076 | -0.1474\n",
      "        13 |            6 |          0 | Stand      | -0.1514 | -0.3095\n",
      "        13 |            6 |          1 | Hit        |  0.0183 |  0.1244\n",
      "        13 |           10 |          0 | Hit        | -0.6062 | -0.4541\n",
      "        13 |           10 |          1 | Hit        | -0.6065 | -0.1466\n",
      "        14 |            1 |          0 | Hit        | -0.7266 | -0.5684\n",
      "        14 |            1 |          1 | Hit        | -0.3345 | -0.2256\n",
      "        14 |            6 |          0 | Stand      | -0.2293 | -0.3084\n",
      "        14 |            6 |          1 | Hit        | -0.0292 |  0.0541\n",
      "        14 |           10 |          0 | Hit        | -0.5865 | -0.4973\n",
      "        14 |           10 |          1 | Hit        | -0.3583 | -0.1927\n",
      "        15 |            1 |          0 | Hit        | -0.7256 | -0.5994\n",
      "        15 |            1 |          1 | Hit        | -0.3042 | -0.2481\n",
      "        15 |            6 |          0 | Stand      | -0.1538 | -0.3871\n",
      "        15 |            6 |          1 | Hit        | -0.1189 |  0.0609\n",
      "        15 |           10 |          0 | Hit        | -0.5933 | -0.5410\n",
      "        15 |           10 |          1 | Hit        | -0.4337 | -0.2221\n",
      "        16 |            1 |          0 | Hit        | -0.6490 | -0.6343\n",
      "        16 |            1 |          1 | Hit        | -0.3216 | -0.3029\n",
      "        16 |            6 |          0 | Stand      | -0.1593 | -0.4821\n",
      "        16 |            6 |          1 | Hit        | -0.0242 |  0.0599\n",
      "        16 |           10 |          0 | Hit        | -0.5875 | -0.5688\n",
      "        16 |           10 |          1 | Hit        | -0.4579 | -0.2610\n",
      "        17 |            1 |          0 | Stand      | -0.6350 | -0.7007\n",
      "        17 |            1 |          1 | Stand      | -0.2535 | -0.3308\n",
      "        17 |            6 |          0 | Stand      |  0.0174 | -0.4623\n",
      "        17 |            6 |          1 | Hit        | -0.1128 |  0.1067\n",
      "        17 |           10 |          0 | Stand      | -0.4694 | -0.5702\n",
      "        17 |           10 |          1 | Hit        | -0.3066 | -0.2381\n",
      "        18 |            1 |          0 | Stand      | -0.3788 | -0.6641\n",
      "        18 |            1 |          1 | Stand      | -0.2547 | -0.2605\n",
      "        18 |            6 |          0 | Stand      |  0.2711 | -0.5797\n",
      "        18 |            6 |          1 | Stand      |  0.2980 |  0.0517\n",
      "        18 |           10 |          0 | Stand      | -0.2392 | -0.6406\n",
      "        18 |           10 |          1 | Hit        | -0.2600 | -0.1891\n",
      "        19 |            1 |          0 | Stand      | -0.1264 | -0.7306\n",
      "        19 |            1 |          1 | Hit        | -0.1090 | -0.0999\n",
      "        19 |            6 |          0 | Stand      |  0.4914 | -0.6951\n",
      "        19 |            6 |          1 | Stand      |  0.4836 |  0.0382\n",
      "        19 |           10 |          0 | Stand      | -0.0216 | -0.7516\n",
      "        19 |           10 |          1 | Stand      | -0.0102 | -0.2280\n",
      "        20 |            1 |          0 | Stand      |  0.1491 | -0.8458\n",
      "        20 |            1 |          1 | Stand      |  0.1670 | -0.0922\n",
      "        20 |            6 |          0 | Stand      |  0.6994 | -0.8514\n",
      "        20 |            6 |          1 | Stand      |  0.6640 | -0.0352\n",
      "        20 |           10 |          0 | Stand      |  0.4399 | -0.8625\n",
      "        20 |           10 |          1 | Stand      |  0.4552 |  0.0061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display policy for critical decision points\n",
    "print(\"\\nLearned Policy (Player Sum vs Dealer Card):\")\n",
    "print(\"Player Sum | Dealer's Card | Usable Ace | Best Action | Q(stand) | Q(hit)\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for player_sum in [12, 13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "    for dealer_card in [1, 6, 10]:  # Dealer showing Ace, 6, 10\n",
    "        for usable_ace in [0, 1]:    # Hard and soft totals\n",
    "            state = (player_sum, dealer_card, usable_ace)\n",
    "            q_values1 = get_q_values(state, Q)\n",
    "            q_values2 = get_q_values(state, Q2)\n",
    "            avg_q_values = (q_values1 + q_values2) / 2\n",
    "            best_action = \"Hit\" if np.argmax(avg_q_values) == 1 else \"Stand\"\n",
    "            print(f\"{player_sum:10d} | {dealer_card:12d} | {usable_ace:10d} | {best_action:10s} | {avg_q_values[0]:7.4f} | {avg_q_values[1]:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average Q-values (ensemble approach)\n",
    "avg_Q = Q.copy()\n",
    "avg_Q['Action 0 (Stand)'] = (Q['Action 0 (Stand)'] + Q2['Action 0 (Stand)']) / 2\n",
    "avg_Q['Action 1 (Hit)'] = (Q['Action 1 (Hit)'] + Q2['Action 1 (Hit)']) / 2\n",
    "avg_Q['Best Action'] = avg_Q.apply(\n",
    "    lambda row: \"Stand\" if row['Action 0 (Stand)'] > row['Action 1 (Hit)'] else \"Hit\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.to_csv('blackjack_q_table1_cpu.csv', index=False)\n",
    "Q2.to_csv('blackjack_q_table2_cpu.csv', index=False)\n",
    "avg_Q.to_csv('blackjack_avg_q_table_cpu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
