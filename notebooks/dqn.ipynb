{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory\n",
    "import os\n",
    "# Python script\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# cwd = os.path.dirname(current_dir)\n",
    "# os.chdir(cwd)\n",
    "# Notebook\n",
    "os.chdir(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from env import BlackjackEnv\n",
    "from visualize import plot_policy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Dqn, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        y = self.fc4(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = transition\n",
    "        self.position = (self.position+1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, agent_config):\n",
    "\n",
    "        self.actions = agent_config[\"actions\"]\n",
    "        self.action_size = len(self.actions)\n",
    "        self.batch_size = agent_config[\"batch_size\"]\n",
    "        self.gamma = agent_config[\"gamma\"]\n",
    "        self.epsilon_max = agent_config[\"epsilon_max\"]\n",
    "        self.current_epsilon = self.epsilon_max\n",
    "        self.epsilon_min = agent_config[\"epsilon_min\"]\n",
    "        self.epsilon_decay = agent_config[\"epsilon_decay\"]\n",
    "        self.memory_size = agent_config[\"memory_size\"]\n",
    "        self.lr = agent_config[\"lr\"]\n",
    "        self.state_size = agent_config[\"state_size\"]\n",
    "\n",
    "        self.policy_net = Dqn(state_size=self.state_size, action_size=self.action_size).to(DEVICE)\n",
    "        self.target_net = Dqn(state_size=self.state_size, action_size=self.action_size).to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # self.criterion = nn.HuberLoss()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.optimizer,\n",
    "            step_size=10000,\n",
    "            gamma=0.95)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.memory_size)\n",
    "    \n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        if state[0]:\n",
    "            state = np.array([\n",
    "                state[0] / 21.0,\n",
    "                state[1] / 10.0,\n",
    "                state[2],\n",
    "                state[3]\n",
    "            ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "\n",
    "    def select_action(self, env, state):\n",
    "        if random.random() < self.current_epsilon:\n",
    "            return env.move_space.sample()\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        self.current_epsilon = max(self.epsilon_min, self.epsilon_max * (self.epsilon_decay**episode))\n",
    "    \n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(DEVICE)\n",
    "        actions = torch.LongTensor(actions).to(DEVICE)\n",
    "        rewards = torch.FloatTensor(rewards).to(DEVICE)\n",
    "        next_states = torch.FloatTensor(next_states).to(DEVICE)\n",
    "        dones = torch.LongTensor(dones).to(DEVICE)\n",
    "\n",
    "        # Compute the target Q-values\n",
    "        Q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        # Use the Bellman equation to update the Q-values\n",
    "        # Q(s,a) = r + Î³ * max_a'(Q(s',a'))\n",
    "        max_a_Q_prime = self.target_net(next_states).max(1)[0].detach()\n",
    "        new_Q = rewards + (1-dones) * self.gamma * max_a_Q_prime\n",
    "\n",
    "        # Compute the loss and update the network\n",
    "        loss = self.criterion(Q, new_Q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_decks': 6,\n",
       " 'cut_card_position': 0.2,\n",
       " 'bets': [1],\n",
       " 'actions': ['stand', 'hit'],\n",
       " 'num_players': 1,\n",
       " 'bet_space': Discrete(1),\n",
       " 'move_space': Discrete(2),\n",
       " 'observation_space': Tuple(Discrete(32), Discrete(11), Discrete(2), Box(0.0, 1.0, (11,), float64)),\n",
       " 'table': <blackjack.Table at 0x188a12a6600>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "ENV_CONFIG = {\n",
    "    \"num_decks\"         : 6,\n",
    "    \"red_card_position\" : 0.2,\n",
    "    \"bet_size\"          : [1],\n",
    "    \"actions\"           : [\"stand\", \"hit\"],\n",
    "    \"num_players\"       : 1\n",
    "}\n",
    "\n",
    "env = BlackjackEnv(config=ENV_CONFIG)\n",
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': ['stand', 'hit'],\n",
       " 'action_size': 2,\n",
       " 'batch_size': 512,\n",
       " 'gamma': 0.999,\n",
       " 'epsilon_max': 1.0,\n",
       " 'current_epsilon': 1.0,\n",
       " 'epsilon_min': 0.01,\n",
       " 'epsilon_decay': 0.9995,\n",
       " 'memory_size': 20000,\n",
       " 'lr': 0.1,\n",
       " 'state_size': 4,\n",
       " 'policy_net': Dqn(\n",
       "   (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
       "   (relu1): ReLU()\n",
       "   (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "   (relu2): ReLU()\n",
       "   (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "   (relu3): ReLU()\n",
       "   (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       " ),\n",
       " 'target_net': Dqn(\n",
       "   (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
       "   (relu1): ReLU()\n",
       "   (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "   (relu2): ReLU()\n",
       "   (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "   (relu3): ReLU()\n",
       "   (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       " ),\n",
       " 'criterion': MSELoss(),\n",
       " 'optimizer': SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     differentiable: False\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     initial_lr: 0.1\n",
       "     lr: 0.1\n",
       "     maximize: False\n",
       "     momentum: 0.9\n",
       "     nesterov: False\n",
       "     weight_decay: 0.0001\n",
       " ),\n",
       " 'scheduler': <torch.optim.lr_scheduler.StepLR at 0x18899fd5010>,\n",
       " 'replay_buffer': <__main__.ReplayBuffer at 0x188a4ac9700>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "AGENT_CONFIG = {\n",
    "    \"actions\"      : [\"stand\", \"hit\"],\n",
    "    \"batch_size\"   : 512,\n",
    "    \"gamma\"        : 0.999,\n",
    "    \"epsilon_max\"  : 1.0,\n",
    "    \"epsilon_min\"  : 0.01,\n",
    "    \"epsilon_decay\": 0.9995,\n",
    "    \"memory_size\"  : 20000,\n",
    "    \"lr\"           : 0.1,\n",
    "    \"state_size\"   : 4\n",
    "}\n",
    "\n",
    "agent = DqnAgent(agent_config=AGENT_CONFIG)\n",
    "agent.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"architecture\"  : \"\",\n",
    "    \"num_episodes\"  : 20000,\n",
    "    \"verbose\"       : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gipop\\AppData\\Local\\Temp\\ipykernel_26384\\3050446909.py:69: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states = torch.FloatTensor(states).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAvg reward: -0.3325,\tWin rate: 0.3010,\tDraw rate: 0.0440,\tLoss rate: 0.6560\n",
      "\t\tLoss: 0.2437,\t\tLr: 0.1000,\t\tEpsilon: 0.6065\n",
      "Episode 2000\tAvg reward: -0.1815,\tWin rate: 0.3385,\tDraw rate: 0.0460,\tLoss rate: 0.6160\n",
      "\t\tLoss: 0.4361,\t\tLr: 0.1000,\t\tEpsilon: 0.3678\n",
      "Episode 3000\tAvg reward: -0.1440,\tWin rate: 0.3547,\tDraw rate: 0.0507,\tLoss rate: 0.5950\n",
      "\t\tLoss: 0.4639,\t\tLr: 0.1000,\t\tEpsilon: 0.2230\n",
      "Episode 4000\tAvg reward: -0.0685,\tWin rate: 0.3733,\tDraw rate: 0.0498,\tLoss rate: 0.5773\n",
      "\t\tLoss: 0.4711,\t\tLr: 0.1000,\t\tEpsilon: 0.1353\n",
      "Episode 5000\tAvg reward: -0.0460,\tWin rate: 0.3840,\tDraw rate: 0.0550,\tLoss rate: 0.5612\n",
      "\t\tLoss: 0.4812,\t\tLr: 0.1000,\t\tEpsilon: 0.0820\n",
      "Episode 6000\tAvg reward: -0.1055,\tWin rate: 0.3877,\tDraw rate: 0.0568,\tLoss rate: 0.5557\n",
      "\t\tLoss: 0.4881,\t\tLr: 0.1000,\t\tEpsilon: 0.0497\n",
      "Episode 7000\tAvg reward: -0.0250,\tWin rate: 0.3949,\tDraw rate: 0.0593,\tLoss rate: 0.5460\n",
      "\t\tLoss: 0.4843,\t\tLr: 0.1000,\t\tEpsilon: 0.0302\n",
      "Episode 8000\tAvg reward: -0.0700,\tWin rate: 0.3980,\tDraw rate: 0.0600,\tLoss rate: 0.5421\n",
      "\t\tLoss: 0.4850,\t\tLr: 0.1000,\t\tEpsilon: 0.0183\n",
      "Episode 9000\tAvg reward: -0.0665,\tWin rate: 0.4013,\tDraw rate: 0.0598,\tLoss rate: 0.5390\n",
      "\t\tLoss: 0.5032,\t\tLr: 0.0950,\t\tEpsilon: 0.0111\n",
      "Episode 10000\tAvg reward: -0.0760,\tWin rate: 0.4031,\tDraw rate: 0.0606,\tLoss rate: 0.5364\n",
      "\t\tLoss: 0.4983,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 11000\tAvg reward: -0.0550,\tWin rate: 0.4060,\tDraw rate: 0.0605,\tLoss rate: 0.5336\n",
      "\t\tLoss: 0.5029,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 12000\tAvg reward: 0.0280,\tWin rate: 0.4116,\tDraw rate: 0.0607,\tLoss rate: 0.5278\n",
      "\t\tLoss: 0.5145,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 13000\tAvg reward: -0.0690,\tWin rate: 0.4120,\tDraw rate: 0.0618,\tLoss rate: 0.5263\n",
      "\t\tLoss: 0.5093,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 14000\tAvg reward: -0.0155,\tWin rate: 0.4146,\tDraw rate: 0.0623,\tLoss rate: 0.5232\n",
      "\t\tLoss: 0.5127,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 15000\tAvg reward: -0.0725,\tWin rate: 0.4144,\tDraw rate: 0.0634,\tLoss rate: 0.5223\n",
      "\t\tLoss: 0.5075,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 16000\tAvg reward: -0.0730,\tWin rate: 0.4153,\tDraw rate: 0.0628,\tLoss rate: 0.5220\n",
      "\t\tLoss: 0.5066,\t\tLr: 0.0950,\t\tEpsilon: 0.0100\n",
      "Episode 17000\tAvg reward: -0.0380,\tWin rate: 0.4167,\tDraw rate: 0.0626,\tLoss rate: 0.5207\n",
      "\t\tLoss: 0.5161,\t\tLr: 0.0902,\t\tEpsilon: 0.0100\n",
      "Episode 18000\tAvg reward: -0.0390,\tWin rate: 0.4181,\tDraw rate: 0.0622,\tLoss rate: 0.5198\n",
      "\t\tLoss: 0.5184,\t\tLr: 0.0902,\t\tEpsilon: 0.0100\n",
      "Episode 19000\tAvg reward: -0.0420,\tWin rate: 0.4192,\tDraw rate: 0.0620,\tLoss rate: 0.5189\n",
      "\t\tLoss: 0.5191,\t\tLr: 0.0902,\t\tEpsilon: 0.0100\n"
     ]
    }
   ],
   "source": [
    "verbose = PARAMS[\"verbose\"]\n",
    "rewards_money = []\n",
    "losses_money = []\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "\n",
    "for episode in range(PARAMS[\"num_episodes\"]):\n",
    "    if verbose:\n",
    "        print(\"-------------------- Starting episode\", episode+1)\n",
    "    state = env.reset()\n",
    "    state = agent.preprocess_state(state)\n",
    "    done = False\n",
    "    episode_loss = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True count: {state[-1]:.4f}\")\n",
    "\n",
    "    # Place a random bet\n",
    "    # TODO: introduce another network to predict the bet size\n",
    "    bet_action = env.bet_space.sample()\n",
    "    state, reward, done = env.step(bet_action, action_type=\"bet\")\n",
    "    if verbose:\n",
    "        print(f\"----- Bet: {env.bets[bet_action]}\")\n",
    "    state = agent.preprocess_state(state)\n",
    "\n",
    "    if verbose:\n",
    "        print(env.table.players[0])\n",
    "        print(env.table.dealer)\n",
    "    if not done:\n",
    "        if verbose:\n",
    "            print(\"----- Making moves...\")\n",
    "        \n",
    "        # Continue until the episode is done\n",
    "        while not done:\n",
    "            action = agent.select_action(env, state)\n",
    "            next_state, reward, done = env.step(action, action_type=\"move\")\n",
    "            next_state = agent.preprocess_state(next_state)\n",
    "\n",
    "            agent.replay_buffer.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            loss = agent.train_step()\n",
    "            episode_loss.append(loss if loss is not None else 0)\n",
    "\n",
    "        if verbose:\n",
    "            print(env.table.players[0])\n",
    "            print(env.table.dealer)\n",
    "    if verbose:\n",
    "        print(f\"----- Reward: {reward}\")\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward == -1:\n",
    "        losses += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # Update the target network\n",
    "        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        agent.update_epsilon(episode)\n",
    "    \n",
    "    # print(episode_loss)\n",
    "    rewards_money.append(reward)\n",
    "    losses_money.append(np.mean(episode_loss).item() if episode_loss else 0)\n",
    "    if verbose:\n",
    "        print(\"-------------------- Terminated\")\n",
    "\n",
    "    if episode > 0 and episode % 1000 == 0:\n",
    "        avg_reward = np.mean(rewards_money[-1000:])\n",
    "        avg_loss = np.mean(losses_money[-1000:])\n",
    "        curr_lr = float(agent.optimizer.param_groups[0][\"lr\"])\n",
    "        print(f\"Episode {episode}\\tAvg reward: {avg_reward:.4f},\\tWin rate: {wins/episode:.4f},\\tDraw rate: {draws/episode:.4f},\\tLoss rate: {losses/episode:.4f}\")\n",
    "        print(f\"\\t\\tLoss: {avg_loss:.4f},\\t\\tLr: {curr_lr:.4f},\\t\\tEpsilon: {agent.current_epsilon:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Average Reward: -0.3758\n",
      "Win Rate: 0.2784\n",
      "Draw Rate: 0.0456\n",
      "Loss Rate: 0.6760\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "env = BlackjackEnv(config=ENV_CONFIG)\n",
    "total_rewards = []\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    state = env.reset()\n",
    "    state = agent.preprocess_state(state)\n",
    "    done = False\n",
    "\n",
    "    bet_action = env.bet_space.sample()\n",
    "    state, reward, done = env.step(bet_action, action_type=\"bet\")\n",
    "    state = agent.preprocess_state(state)\n",
    "\n",
    "    if not done:\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "                action = agent.policy_net(state_tensor).argmax().item()\n",
    "                \n",
    "            next_state, reward, done = env.step(action, action_type=\"move\")\n",
    "            next_state = agent.preprocess_state(next_state)\n",
    "    \n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward == -1:\n",
    "        losses += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "    total_rewards.append(reward)\n",
    "\n",
    "avg_reward = np.mean(total_rewards)\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "print(f\"Win Rate: {wins/10000:.4f}\")\n",
    "print(f\"Draw Rate: {draws/10000:.4f}\")\n",
    "print(f\"Loss Rate: {losses/10000:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_strategy(agent, file_path):\n",
    "\n",
    "    strategy = pd.DataFrame(columns=[\"State\", \"Action\"])\n",
    "    \n",
    "    for player_hand in range(4, 22):\n",
    "        for dealer_hand in range(2, 12):\n",
    "            for soft_hand in range(2):\n",
    "                for true_count in range(-4, 5):\n",
    "                    state = (player_hand, dealer_hand, soft_hand, true_count)\n",
    "                    actions = []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        state = torch.FloatTensor([player_hand, dealer_hand, soft_hand, true_count]).unsqueeze(0).to(DEVICE)\n",
    "                        q_values = agent.policy_net(state).argmax().item()\n",
    "                        actions.append(q_values)\n",
    "\n",
    "                state = f\"({player_hand}, {dealer_hand}, {soft_hand})\"\n",
    "                action = np.mean(actions).item()\n",
    "                strategy = strategy.append({\"State\": state, \"Action\": action}, ignore_index=True)\n",
    "    \n",
    "    strategy.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [],
   "source": [
    "save_strategy(agent, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-21-brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
